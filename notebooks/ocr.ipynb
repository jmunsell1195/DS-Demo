{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmunse/anaconda3/envs/test/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrOCRForCausalLM, TrOCRConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = TrOCRConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TrOCRForCausalLM(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on TrOCRForCausalLM in module transformers.models.trocr.modeling_trocr object:\n",
      "\n",
      "class TrOCRForCausalLM(TrOCRPreTrainedModel)\n",
      " |  TrOCRForCausalLM(config)\n",
      " |  \n",
      " |  The TrOCR Decoder with a language modeling head. Can be used as the decoder part of [`EncoderDecoderModel`] and [`VisionEncoderDecoder`].\n",
      " |  This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n",
      " |  library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n",
      " |  etc.)\n",
      " |  \n",
      " |  This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n",
      " |  Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n",
      " |  and behavior.\n",
      " |  \n",
      " |  Parameters:\n",
      " |      config ([`TrOCRConfig`]):\n",
      " |          Model configuration class with all the parameters of the model. Initializing with a config file does not\n",
      " |          load the weights associated with the model, only the configuration. Check out the\n",
      " |          [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      TrOCRForCausalLM\n",
      " |      TrOCRPreTrainedModel\n",
      " |      transformers.modeling_utils.PreTrainedModel\n",
      " |      torch.nn.modules.module.Module\n",
      " |      transformers.modeling_utils.ModuleUtilsMixin\n",
      " |      transformers.generation_utils.GenerationMixin\n",
      " |      transformers.utils.hub.PushToHubMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, config)\n",
      " |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      " |  \n",
      " |  forward(self, input_ids=None, attention_mask=None, encoder_hidden_states=None, encoder_attention_mask=None, head_mask=None, cross_attn_head_mask=None, past_key_values=None, inputs_embeds=None, labels=None, use_cache=None, output_attentions=None, output_hidden_states=None, return_dict=None)\n",
      " |      Args:\n",
      " |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      " |              Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n",
      " |              provide it.\n",
      " |      \n",
      " |              Indices can be obtained using [`TrOCRTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
      " |              [`PreTrainedTokenizer.__call__`] for details.\n",
      " |      \n",
      " |              [What are input IDs?](../glossary#input-ids)\n",
      " |          attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      " |              Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
      " |      \n",
      " |              - 1 for tokens that are **not masked**,\n",
      " |              - 0 for tokens that are **masked**.\n",
      " |      \n",
      " |              [What are attention masks?](../glossary#attention-mask)\n",
      " |          encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
      " |              Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention\n",
      " |              if the model is configured as a decoder.\n",
      " |          encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      " |              Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used\n",
      " |              in the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n",
      " |          head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n",
      " |              Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n",
      " |      \n",
      " |              - 1 indicates the head is **not masked**,\n",
      " |              - 0 indicates the head is **masked**.\n",
      " |      \n",
      " |          cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n",
      " |              Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:\n",
      " |      \n",
      " |              - 1 indicates the head is **not masked**,\n",
      " |              - 0 indicates the head is **masked**.\n",
      " |      \n",
      " |          past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
      " |              Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n",
      " |              shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n",
      " |              shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`. The two additional\n",
      " |              tensors are only required when the model is used as a decoder in a Sequence to Sequence model.\n",
      " |      \n",
      " |              Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n",
      " |              cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n",
      " |      \n",
      " |              If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those\n",
      " |              that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of\n",
      " |              all `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n",
      " |          labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      " |              Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
      " |              config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
      " |              (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
      " |          use_cache (`bool`, *optional*):\n",
      " |              If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n",
      " |              (see `past_key_values`).\n",
      " |      \n",
      " |              - 1 for tokens that are **not masked**,\n",
      " |              - 0 for tokens that are **masked**.\n",
      " |          output_attentions (`bool`, *optional*):\n",
      " |              Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
      " |              returned tensors for more detail.\n",
      " |          output_hidden_states (`bool`, *optional*):\n",
      " |              Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
      " |              for more detail.\n",
      " |          return_dict (`bool`, *optional*):\n",
      " |              Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
      " |      \n",
      " |      \n",
      " |      Returns:\n",
      " |          [`transformers.modeling_outputs.CausalLMOutputWithCrossAttentions`] or `tuple(torch.FloatTensor)`: A [`transformers.modeling_outputs.CausalLMOutputWithCrossAttentions`] or a tuple of\n",
      " |          `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`) comprising various\n",
      " |          elements depending on the configuration ([`TrOCRConfig`]) and inputs.\n",
      " |      \n",
      " |          - **loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided) -- Language modeling loss (for next-token prediction).\n",
      " |          - **logits** (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`) -- Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n",
      " |          - **hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) -- Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n",
      " |            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n",
      " |      \n",
      " |            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n",
      " |          - **attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
      " |            sequence_length)`.\n",
      " |      \n",
      " |            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
      " |            heads.\n",
      " |          - **cross_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
      " |            sequence_length)`.\n",
      " |      \n",
      " |            Cross attentions weights after the attention softmax, used to compute the weighted average in the\n",
      " |            cross-attention heads.\n",
      " |          - **past_key_values** (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`) -- Tuple of `torch.FloatTensor` tuples of length `config.n_layers`, with each tuple containing the cached key,\n",
      " |            value states of the self-attention and the cross-attention layers if model is used in encoder-decoder\n",
      " |            setting. Only relevant if `config.is_decoder = True`.\n",
      " |      \n",
      " |            Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see\n",
      " |            `past_key_values` input) to speed up sequential decoding.\n",
      " |      \n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      >>> from transformers import (\n",
      " |      ...     TrOCRConfig,\n",
      " |      ...     TrOCRProcessor,\n",
      " |      ...     TrOCRForCausalLM,\n",
      " |      ...     ViTConfig,\n",
      " |      ...     ViTModel,\n",
      " |      ...     VisionEncoderDecoderModel,\n",
      " |      ... )\n",
      " |      >>> import requests\n",
      " |      >>> from PIL import Image\n",
      " |      \n",
      " |      >>> # TrOCR is a decoder model and should be used within a VisionEncoderDecoderModel\n",
      " |      >>> # init vision2text model with random weights\n",
      " |      >>> encoder = ViTModel(ViTConfig())\n",
      " |      >>> decoder = TrOCRForCausalLM(TrOCRConfig())\n",
      " |      >>> model = VisionEncoderDecoderModel(encoder=encoder, decoder=decoder)\n",
      " |      \n",
      " |      >>> # If you want to start from the pretrained model, load the checkpoint with `VisionEncoderDecoderModel`\n",
      " |      >>> processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
      " |      >>> model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
      " |      \n",
      " |      >>> # load image from the IAM dataset\n",
      " |      >>> url = \"https://fki.tic.heia-fr.ch/static/img/a01-122-02.jpg\"\n",
      " |      >>> image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n",
      " |      >>> pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
      " |      >>> text = \"industry, ' Mr. Brown commented icily. ' Let us have a\"\n",
      " |      \n",
      " |      >>> # training\n",
      " |      >>> model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
      " |      >>> model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
      " |      >>> model.config.vocab_size = model.config.decoder.vocab_size\n",
      " |      \n",
      " |      >>> labels = processor.tokenizer(text, return_tensors=\"pt\").input_ids\n",
      " |      >>> outputs = model(pixel_values, labels=labels)\n",
      " |      >>> loss = outputs.loss\n",
      " |      >>> round(loss.item(), 2)\n",
      " |      5.30\n",
      " |      \n",
      " |      >>> # inference\n",
      " |      >>> generated_ids = model.generate(pixel_values)\n",
      " |      >>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
      " |      >>> generated_text\n",
      " |      'industry, \" Mr. Brown commented icily. \" Let us have a'\n",
      " |      ```\n",
      " |  \n",
      " |  get_decoder(self)\n",
      " |  \n",
      " |  get_input_embeddings(self)\n",
      " |      Returns the model's input embeddings.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `nn.Module`: A torch module mapping vocabulary to hidden states.\n",
      " |  \n",
      " |  get_output_embeddings(self)\n",
      " |      Returns the model's output embeddings.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `nn.Module`: A torch module mapping hidden states to vocabulary.\n",
      " |  \n",
      " |  prepare_inputs_for_generation(self, input_ids, past=None, attention_mask=None, use_cache=None, **kwargs)\n",
      " |      Implement in subclasses of [`PreTrainedModel`] for custom behavior to prepare inputs in the generate method.\n",
      " |  \n",
      " |  set_decoder(self, decoder)\n",
      " |  \n",
      " |  set_input_embeddings(self, value)\n",
      " |      Set model's input embeddings.\n",
      " |      \n",
      " |      Args:\n",
      " |          value (`nn.Module`): A module mapping vocabulary to hidden states.\n",
      " |  \n",
      " |  set_output_embeddings(self, new_embeddings)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from TrOCRPreTrainedModel:\n",
      " |  \n",
      " |  base_model_prefix = 'model'\n",
      " |  \n",
      " |  config_class = <class 'transformers.models.trocr.configuration_trocr.T...\n",
      " |      This is the configuration class to store the configuration of a [`TrOCRForCausalLM`]. It is used to instantiate an\n",
      " |      TrOCR model according to the specified arguments, defining the model architecture. Instantiating a configuration\n",
      " |      with the defaults will yield a similar configuration to that of the TrOCR\n",
      " |      [microsoft/trocr-base-handwritten](https://huggingface.co/microsoft/trocr-base-handwritten) architecture.\n",
      " |      \n",
      " |      Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n",
      " |      documentation from [`PretrainedConfig`] for more information.\n",
      " |      \n",
      " |      \n",
      " |      Args:\n",
      " |          vocab_size (`int`, *optional*, defaults to 50265):\n",
      " |              Vocabulary size of the TrOCR model. Defines the number of different tokens that can be represented by the\n",
      " |              `inputs_ids` passed when calling [`TrOCRForCausalLM`].\n",
      " |          d_model (`int`, *optional*, defaults to 1024):\n",
      " |              Dimensionality of the layers and the pooler layer.\n",
      " |          decoder_layers (`int`, *optional*, defaults to 12):\n",
      " |              Number of decoder layers.\n",
      " |          decoder_attention_heads (`int`, *optional*, defaults to 16):\n",
      " |              Number of attention heads for each attention layer in the Transformer decoder.\n",
      " |          decoder_ffn_dim (`int`, *optional*, defaults to 4096):\n",
      " |              Dimensionality of the \"intermediate\" (often named feed-forward) layer in decoder.\n",
      " |          activation_function (`str` or `function`, *optional*, defaults to `\"gelu\"`):\n",
      " |              The non-linear activation function (function or string) in the pooler. If string, `\"gelu\"`, `\"relu\"`,\n",
      " |              `\"silu\"` and `\"gelu_new\"` are supported.\n",
      " |          max_position_embeddings (`int`, *optional*, defaults to 512):\n",
      " |              The maximum sequence length that this model might ever be used with. Typically set this to something large\n",
      " |              just in case (e.g., 512 or 1024 or 2048).\n",
      " |          dropout (`float`, *optional*, defaults to 0.1):\n",
      " |              The dropout probability for all fully connected layers in the embeddings, and pooler.\n",
      " |          attention_dropout (`float`, *optional*, defaults to 0.0):\n",
      " |              The dropout ratio for the attention probabilities.\n",
      " |          activation_dropout (`float`, *optional*, defaults to 0.0):\n",
      " |              The dropout ratio for activations inside the fully connected layer.\n",
      " |          classifier_dropout (`float`, *optional*, defaults to 0.0):\n",
      " |              The dropout ratio for classifier.\n",
      " |          init_std (`float`, *optional*, defaults to 0.02):\n",
      " |              The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n",
      " |          decoder_layerdrop (`float`, *optional*, defaults to 0.0):\n",
      " |              The LayerDrop probability for the decoder. See the [LayerDrop paper](see https://arxiv.org/abs/1909.11556)\n",
      " |              for more details.\n",
      " |          use_cache (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not the model should return the last key/values attentions (not used by all models).\n",
      " |          scale_embedding (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to scale the word embeddings by sqrt(d_model).\n",
      " |          use_learned_position_embeddings (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to use learned position embeddings. If not, sinusoidal position embeddings will be used.\n",
      " |          layernorm_embedding (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to use a layernorm after the word + position embeddings.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      >>> from transformers import TrOCRForCausalLM, TrOCRConfig\n",
      " |      \n",
      " |      >>> # Initializing a TrOCR-base style configuration\n",
      " |      >>> configuration = TrOCRConfig()\n",
      " |      \n",
      " |      >>> # Initializing a model from the TrOCR-base style configuration\n",
      " |      >>> model = TrOCRForCausalLM(configuration)\n",
      " |      \n",
      " |      >>> # Accessing the model configuration\n",
      " |      >>> configuration = model.config\n",
      " |      ```\n",
      " |  \n",
      " |  \n",
      " |  supports_gradient_checkpointing = True\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.modeling_utils.PreTrainedModel:\n",
      " |  \n",
      " |  get_memory_footprint(self, return_buffers=True)\n",
      " |      Get the memory footprint of a model. This will return the memory footprint of the current model in bytes.\n",
      " |      Useful to benchmark the memory footprint of the current model and design some tests. Solution inspired from the\n",
      " |      PyTorch discussions: https://discuss.pytorch.org/t/gpu-memory-that-model-uses/56822/2\n",
      " |      \n",
      " |      Arguments:\n",
      " |          return_buffers (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether to return the size of the buffer tensors in the computation of the memory footprint. Buffers\n",
      " |              are tensors that do not require gradients and not registered as parameters. E.g. mean and std in batch\n",
      " |              norm layers. Please see: https://discuss.pytorch.org/t/what-pytorch-means-by-buffers/120266/2\n",
      " |  \n",
      " |  get_position_embeddings(self) -> Union[torch.nn.modules.sparse.Embedding, Tuple[torch.nn.modules.sparse.Embedding]]\n",
      " |  \n",
      " |  gradient_checkpointing_disable(self)\n",
      " |      Deactivates gradient checkpointing for the current model.\n",
      " |      \n",
      " |      Note that in other frameworks this feature can be referred to as \"activation checkpointing\" or \"checkpoint\n",
      " |      activations\".\n",
      " |  \n",
      " |  gradient_checkpointing_enable(self)\n",
      " |      Activates gradient checkpointing for the current model.\n",
      " |      \n",
      " |      Note that in other frameworks this feature can be referred to as \"activation checkpointing\" or \"checkpoint\n",
      " |      activations\".\n",
      " |  \n",
      " |  init_weights(self)\n",
      " |      If needed prunes and maybe initializes weights.\n",
      " |  \n",
      " |  post_init(self)\n",
      " |      A method executed at the end of each Transformer model initialization, to execute code that needs the model's\n",
      " |      modules properly initialized (such as weight initialization).\n",
      " |  \n",
      " |  prune_heads(self, heads_to_prune: Dict[int, List[int]])\n",
      " |      Prunes heads of the base model.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          heads_to_prune (`Dict[int, List[int]]`):\n",
      " |              Dictionary with keys being selected layer indices (`int`) and associated values being the list of heads\n",
      " |              to prune in said layer (list of `int`). For instance {1: [0, 2], 2: [2, 3]} will prune heads 0 and 2 on\n",
      " |              layer 1 and heads 2 and 3 on layer 2.\n",
      " |  \n",
      " |  push_to_hub(self, repo_id: str, use_temp_dir: Optional[bool] = None, commit_message: Optional[str] = None, private: Optional[bool] = None, use_auth_token: Union[bool, str, NoneType] = None, max_shard_size: Union[int, str, NoneType] = '10GB', create_pr: bool = False, **deprecated_kwargs) -> str\n",
      " |      Upload the model file to the 🤗 Model Hub while synchronizing a local clone of the repo in\n",
      " |      `repo_path_or_name`.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          repo_id (`str`):\n",
      " |              The name of the repository you want to push your model to. It should contain your organization name\n",
      " |              when pushing to a given organization.\n",
      " |          use_temp_dir (`bool`, *optional*):\n",
      " |              Whether or not to use a temporary directory to store the files saved before they are pushed to the Hub.\n",
      " |              Will default to `True` if there is no directory named like `repo_id`, `False` otherwise.\n",
      " |          commit_message (`str`, *optional*):\n",
      " |              Message to commit while pushing. Will default to `\"Upload model\"`.\n",
      " |          private (`bool`, *optional*):\n",
      " |              Whether or not the repository created should be private (requires a paying subscription).\n",
      " |          use_auth_token (`bool` or `str`, *optional*):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n",
      " |              when running `huggingface-cli login` (stored in `~/.huggingface`). Will default to `True` if `repo_url`\n",
      " |              is not specified.\n",
      " |          max_shard_size (`int` or `str`, *optional*, defaults to `\"10GB\"`):\n",
      " |              Only applicable for models. The maximum size for a checkpoint before being sharded. Checkpoints shard\n",
      " |              will then be each of size lower than this size. If expressed as a string, needs to be digits followed\n",
      " |              by a unit (like `\"5MB\"`).\n",
      " |          create_pr (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to create a PR with the uploaded files or directly commit.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      from transformers import AutoModel\n",
      " |      \n",
      " |      model = AutoModel.from_pretrained(\"bert-base-cased\")\n",
      " |      \n",
      " |      # Push the model to your namespace with the name \"my-finetuned-bert\".\n",
      " |      model.push_to_hub(\"my-finetuned-bert\")\n",
      " |      \n",
      " |      # Push the model to an organization with the name \"my-finetuned-bert\".\n",
      " |      model.push_to_hub(\"huggingface/my-finetuned-bert\")\n",
      " |      ```\n",
      " |  \n",
      " |  resize_position_embeddings(self, new_num_position_embeddings: int)\n",
      " |  \n",
      " |  resize_token_embeddings(self, new_num_tokens: Optional[int] = None) -> torch.nn.modules.sparse.Embedding\n",
      " |      Resizes input token embeddings matrix of the model if `new_num_tokens != config.vocab_size`.\n",
      " |      \n",
      " |      Takes care of tying weights embeddings afterwards if the model class has a `tie_weights()` method.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          new_num_tokens (`int`, *optional*):\n",
      " |              The number of new tokens in the embedding matrix. Increasing the size will add newly initialized\n",
      " |              vectors at the end. Reducing the size will remove vectors from the end. If not provided or `None`, just\n",
      " |              returns a pointer to the input tokens `torch.nn.Embedding` module of the model without doing anything.\n",
      " |      \n",
      " |      Return:\n",
      " |          `torch.nn.Embedding`: Pointer to the input tokens Embeddings Module of the model.\n",
      " |  \n",
      " |  retrieve_modules_from_names(self, names, add_prefix=False, remove_prefix=False)\n",
      " |  \n",
      " |  save_pretrained(self, save_directory: Union[str, os.PathLike], is_main_process: bool = True, state_dict: Optional[dict] = None, save_function: Callable = <function save at 0x7fee33f40940>, push_to_hub: bool = False, max_shard_size: Union[int, str] = '10GB', **kwargs)\n",
      " |      Save a model and its configuration file to a directory, so that it can be re-loaded using the\n",
      " |      `[`~PreTrainedModel.from_pretrained`]` class method.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          save_directory (`str` or `os.PathLike`):\n",
      " |              Directory to which to save. Will be created if it doesn't exist.\n",
      " |          is_main_process (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether the process calling this is the main process or not. Useful when in distributed training like\n",
      " |              TPUs and need to call this function on all processes. In this case, set `is_main_process=True` only on\n",
      " |              the main process to avoid race conditions.\n",
      " |          state_dict (nested dictionary of `torch.Tensor`):\n",
      " |              The state dictionary of the model to save. Will default to `self.state_dict()`, but can be used to only\n",
      " |              save parts of the model or if special precautions need to be taken when recovering the state dictionary\n",
      " |              of a model (like when using model parallelism).\n",
      " |          save_function (`Callable`):\n",
      " |              The function to use to save the state dictionary. Useful on distributed training like TPUs when one\n",
      " |              need to replace `torch.save` by another method.\n",
      " |          push_to_hub (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the\n",
      " |              repository you want to push to with `repo_id` (will default to the name of `save_directory` in your\n",
      " |              namespace).\n",
      " |          max_shard_size (`int` or `str`, *optional*, defaults to `\"10GB\"`):\n",
      " |              The maximum size for a checkpoint before being sharded. Checkpoints shard will then be each of size\n",
      " |              lower than this size. If expressed as a string, needs to be digits followed by a unit (like `\"5MB\"`).\n",
      " |      \n",
      " |              <Tip warning={true}>\n",
      " |      \n",
      " |              If a single weight of the model is bigger than `max_shard_size`, it will be in its own checkpoint shard\n",
      " |              which will be bigger than `max_shard_size`.\n",
      " |      \n",
      " |              </Tip>\n",
      " |      \n",
      " |          kwargs:\n",
      " |              Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n",
      " |  \n",
      " |  tie_weights(self)\n",
      " |      Tie the weights between the input embeddings and the output embeddings.\n",
      " |      \n",
      " |      If the `torchscript` flag is set in the configuration, can't handle parameter sharing so we are cloning the\n",
      " |      weights instead.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from transformers.modeling_utils.PreTrainedModel:\n",
      " |  \n",
      " |  from_pretrained(pretrained_model_name_or_path: Union[str, os.PathLike, NoneType], *model_args, **kwargs) from builtins.type\n",
      " |      Instantiate a pretrained pytorch model from a pre-trained model configuration.\n",
      " |      \n",
      " |      The model is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated). To train\n",
      " |      the model, you should first set it back in training mode with `model.train()`.\n",
      " |      \n",
      " |      The warning *Weights from XXX not initialized from pretrained model* means that the weights of XXX do not come\n",
      " |      pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning\n",
      " |      task.\n",
      " |      \n",
      " |      The warning *Weights from XXX not used in YYY* means that the layer XXX is not used by YYY, therefore those\n",
      " |      weights are discarded.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          pretrained_model_name_or_path (`str` or `os.PathLike`, *optional*):\n",
      " |              Can be either:\n",
      " |      \n",
      " |                  - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.\n",
      " |                    Valid model ids can be located at the root-level, like `bert-base-uncased`, or namespaced under a\n",
      " |                    user or organization name, like `dbmdz/bert-base-german-cased`.\n",
      " |                  - A path to a *directory* containing model weights saved using\n",
      " |                    [`~PreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.\n",
      " |                  - A path or url to a *tensorflow index checkpoint file* (e.g, `./tf_model/model.ckpt.index`). In\n",
      " |                    this case, `from_tf` should be set to `True` and a configuration object should be provided as\n",
      " |                    `config` argument. This loading path is slower than converting the TensorFlow checkpoint in a\n",
      " |                    PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.\n",
      " |                  - A path or url to a model folder containing a *flax checkpoint file* in *.msgpack* format (e.g,\n",
      " |                    `./flax_model/` containing `flax_model.msgpack`). In this case, `from_flax` should be set to\n",
      " |                    `True`.\n",
      " |                  - `None` if you are both providing the configuration and state dictionary (resp. with keyword\n",
      " |                    arguments `config` and `state_dict`).\n",
      " |          model_args (sequence of positional arguments, *optional*):\n",
      " |              All remaining positional arguments will be passed to the underlying model's `__init__` method.\n",
      " |          config (`Union[PretrainedConfig, str, os.PathLike]`, *optional*):\n",
      " |              Can be either:\n",
      " |      \n",
      " |                  - an instance of a class derived from [`PretrainedConfig`],\n",
      " |                  - a string or path valid as input to [`~PretrainedConfig.from_pretrained`].\n",
      " |      \n",
      " |              Configuration for the model to use instead of an automatically loaded configuration. Configuration can\n",
      " |              be automatically loaded when:\n",
      " |      \n",
      " |                  - The model is a model provided by the library (loaded with the *model id* string of a pretrained\n",
      " |                    model).\n",
      " |                  - The model was saved using [`~PreTrainedModel.save_pretrained`] and is reloaded by supplying the\n",
      " |                    save directory.\n",
      " |                  - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a\n",
      " |                    configuration JSON file named *config.json* is found in the directory.\n",
      " |          state_dict (`Dict[str, torch.Tensor]`, *optional*):\n",
      " |              A state dictionary to use instead of a state dictionary loaded from saved weights file.\n",
      " |      \n",
      " |              This option can be used if you want to create a model from a pretrained configuration but load your own\n",
      " |              weights. In this case though, you should check if using [`~PreTrainedModel.save_pretrained`] and\n",
      " |              [`~PreTrainedModel.from_pretrained`] is not a simpler option.\n",
      " |          cache_dir (`Union[str, os.PathLike]`, *optional*):\n",
      " |              Path to a directory in which a downloaded pretrained model configuration should be cached if the\n",
      " |              standard cache should not be used.\n",
      " |          from_tf (`bool`, *optional*, defaults to `False`):\n",
      " |              Load the model weights from a TensorFlow checkpoint save file (see docstring of\n",
      " |              `pretrained_model_name_or_path` argument).\n",
      " |          from_flax (`bool`, *optional*, defaults to `False`):\n",
      " |              Load the model weights from a Flax checkpoint save file (see docstring of\n",
      " |              `pretrained_model_name_or_path` argument).\n",
      " |          ignore_mismatched_sizes (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to raise an error if some of the weights from the checkpoint do not have the same size\n",
      " |              as the weights of the model (if for instance, you are instantiating a model with 10 labels from a\n",
      " |              checkpoint with 3 labels).\n",
      " |          force_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n",
      " |              cached versions if they exist.\n",
      " |          resume_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to delete incompletely received files. Will attempt to resume the download if such a\n",
      " |              file exists.\n",
      " |          proxies (`Dict[str, str]`, *optional*):\n",
      " |              A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n",
      " |              'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
      " |          output_loading_info(`bool`, *optional*, defaults to `False`):\n",
      " |              Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.\n",
      " |          local_files_only(`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to only look at local files (i.e., do not try to download the model).\n",
      " |          use_auth_token (`str` or *bool*, *optional*):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n",
      " |              when running `huggingface-cli login` (stored in `~/.huggingface`).\n",
      " |          revision (`str`, *optional*, defaults to `\"main\"`):\n",
      " |              The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n",
      " |              git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n",
      " |              identifier allowed by git.\n",
      " |          mirror (`str`, *optional*):\n",
      " |              Mirror source to accelerate downloads in China. If you are from China and have an accessibility\n",
      " |              problem, you can set this option to resolve it. Note that we do not guarantee the timeliness or safety.\n",
      " |              Please refer to the mirror site for more information.\n",
      " |          _fast_init(`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to disable fast initialization.\n",
      " |      \n",
      " |              <Tip warning={true}>\n",
      " |      \n",
      " |              One should only disable *_fast_init* to ensure backwards compatibility with `transformers.__version__ <\n",
      " |              4.6.0` for seeded model initialization. This argument will be removed at the next major version. See\n",
      " |              [pull request 11471](https://github.com/huggingface/transformers/pull/11471) for more information.\n",
      " |      \n",
      " |              </Tip>\n",
      " |      \n",
      " |          > Parameters for big model inference\n",
      " |      \n",
      " |          low_cpu_mem_usage(`bool`, *optional*):\n",
      " |              Tries to not use more than 1x model size in CPU memory (including peak memory) while loading the model.\n",
      " |              This is an experimental feature and a subject to change at any moment.\n",
      " |          torch_dtype (`str` or `torch.dtype`, *optional*):\n",
      " |              Override the default `torch.dtype` and load the model under this dtype. If `\"auto\"` is passed the dtype\n",
      " |              will be automatically derived from the model's weights.\n",
      " |          device_map (`str` or `Dict[str, Union[int, str, torch.device]]`, *optional*):\n",
      " |              A map that specifies where each submodule should go. It doesn't need to be refined to each\n",
      " |              parameter/buffer name, once a given module name is inside, every submodule of it will be sent to the\n",
      " |              same device.\n",
      " |      \n",
      " |              To have Accelerate compute the most optimized `device_map` automatically, set `device_map=\"auto\"`. For\n",
      " |              more information about each option see [designing a device\n",
      " |              map](https://hf.co/docs/accelerate/main/big_modeling#designing-a-device-map).\n",
      " |          max_memory (`Dict`, *optional*):\n",
      " |              A dictionary device identifier to maximum memory. Will default to the maximum memory available for each\n",
      " |              GPU and the available CPU RAM if unset.\n",
      " |          offload_folder (`str` or `os.PathLike`, *optional*):\n",
      " |              If the `device_map` contains any value `\"disk\"`, the folder where we will offload weights.\n",
      " |          offload_state_dict (`bool`, *optional*):\n",
      " |              If `True`, will temporarily offload the CPU state dict to the hard drive to avoid getting out of CPU\n",
      " |              RAM if the weight of the CPU state dict + the biggest shard of the checkpoint does not fit. Defaults to\n",
      " |              `True` when there is some disk offload.\n",
      " |          load_in_8bit (`bool`, *optional*, defaults to `False`):\n",
      " |              If `True`, will convert the loaded model into mixed-8bit quantized model. To use this feature please\n",
      " |              install `bitsandbytes` compiled with your CUDA version by running `pip install -i\n",
      " |              https://test.pypi.org/simple/ bitsandbytes-cudaXXX` where XXX is your CUDA version (e.g. 11.6 = 116).\n",
      " |              Make also sure that you have enough GPU RAM to store half of the model size since the 8bit modules are\n",
      " |              not compiled and adapted for CPUs.\n",
      " |          int8_threshold (`float`, *optional*, defaults to 6):\n",
      " |              Works together with `load_in_8bit`. This corresponds to the outlier threshold for outlier detection as\n",
      " |              described in `GPT3.int8() : 8-bit Matrix Multiplication for Transformers at Scale` paper. Any hidden\n",
      " |              states value that is above this threshold will be considered an outlier and the operation on those\n",
      " |              values will be done in fp16. Values are usually normally distributed, that is, most values are in the\n",
      " |              range [-3.5, 3.5], but there are some exceptional systematic outliers that are very differently\n",
      " |              distributed for large models. These outliers are often in the interval [-60, -6] or [6, 60]. Int8\n",
      " |              quantization works well for values of magnitude ~5, but beyond that, there is a significant performance\n",
      " |              penalty. A good default threshold is 6, but a lower threshold might be needed for more unstable models\n",
      " |              (small models, fine-tuning).\n",
      " |          subfolder (`str`, *optional*, defaults to `\"\"`):\n",
      " |              In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can\n",
      " |              specify the folder name here.\n",
      " |      \n",
      " |          kwargs (remaining dictionary of keyword arguments, *optional*):\n",
      " |              Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,\n",
      " |              `output_attentions=True`). Behaves differently depending on whether a `config` is provided or\n",
      " |              automatically loaded:\n",
      " |      \n",
      " |                  - If a configuration is provided with `config`, `**kwargs` will be directly passed to the\n",
      " |                    underlying model's `__init__` method (we assume all relevant updates to the configuration have\n",
      " |                    already been done)\n",
      " |                  - If a configuration is not provided, `kwargs` will be first passed to the configuration class\n",
      " |                    initialization function ([`~PretrainedConfig.from_pretrained`]). Each key of `kwargs` that\n",
      " |                    corresponds to a configuration attribute will be used to override said attribute with the\n",
      " |                    supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute\n",
      " |                    will be passed to the underlying model's `__init__` function.\n",
      " |      \n",
      " |      <Tip>\n",
      " |      \n",
      " |      Passing `use_auth_token=True`` is required when you want to use a private model.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      <Tip>\n",
      " |      \n",
      " |      Activate the special [\"offline-mode\"](https://huggingface.co/transformers/installation.html#offline-mode) to\n",
      " |      use this method in a firewalled environment.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      >>> from transformers import BertConfig, BertModel\n",
      " |      \n",
      " |      >>> # Download model and configuration from huggingface.co and cache.\n",
      " |      >>> model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
      " |      >>> # Model was saved using *save_pretrained('./test/saved_model/')* (for example purposes, not runnable).\n",
      " |      >>> model = BertModel.from_pretrained(\"./test/saved_model/\")\n",
      " |      >>> # Update configuration during loading.\n",
      " |      >>> model = BertModel.from_pretrained(\"bert-base-uncased\", output_attentions=True)\n",
      " |      >>> assert model.config.output_attentions == True\n",
      " |      >>> # Loading from a TF checkpoint file instead of a PyTorch model (slower, for example purposes, not runnable).\n",
      " |      >>> config = BertConfig.from_json_file(\"./tf_model/my_tf_model_config.json\")\n",
      " |      >>> model = BertModel.from_pretrained(\"./tf_model/my_tf_checkpoint.ckpt.index\", from_tf=True, config=config)\n",
      " |      >>> # Loading from a Flax checkpoint file instead of a PyTorch model (slower)\n",
      " |      >>> model = BertModel.from_pretrained(\"bert-base-uncased\", from_flax=True)\n",
      " |      ```\n",
      " |      \n",
      " |      * `low_cpu_mem_usage` algorithm:\n",
      " |      \n",
      " |      This is an experimental function that loads the model using ~1x model size CPU memory\n",
      " |      \n",
      " |      Here is how it works:\n",
      " |      \n",
      " |      1. save which state_dict keys we have\n",
      " |      2. drop state_dict before the model is created, since the latter takes 1x model size CPU memory\n",
      " |      3. after the model has been instantiated switch to the meta device all params/buffers that\n",
      " |      are going to be replaced from the loaded state_dict\n",
      " |      4. load state_dict 2nd time\n",
      " |      5. replace the params/buffers from the state_dict\n",
      " |      \n",
      " |      Currently, it can't handle deepspeed ZeRO stage 3 and ignores loading errors\n",
      " |  \n",
      " |  register_for_auto_class(auto_class='AutoModel') from builtins.type\n",
      " |      Register this class with a given auto class. This should only be used for custom models as the ones in the\n",
      " |      library are already mapped with an auto class.\n",
      " |      \n",
      " |      <Tip warning={true}>\n",
      " |      \n",
      " |      This API is experimental and may have some slight breaking changes in the next releases.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Args:\n",
      " |          auto_class (`str` or `type`, *optional*, defaults to `\"AutoModel\"`):\n",
      " |              The auto class to register this new model with.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from transformers.modeling_utils.PreTrainedModel:\n",
      " |  \n",
      " |  base_model\n",
      " |      `torch.nn.Module`: The main body of the model.\n",
      " |  \n",
      " |  dummy_inputs\n",
      " |      `Dict[str, torch.Tensor]`: Dummy inputs to do a forward pass in the network.\n",
      " |  \n",
      " |  framework\n",
      " |      :str: Identifies that this is a PyTorch model.\n",
      " |  \n",
      " |  is_gradient_checkpointing\n",
      " |      Whether gradient checkpointing is activated for this model or not.\n",
      " |      \n",
      " |      Note that in other frameworks this feature can be referred to as \"activation checkpointing\" or \"checkpoint\n",
      " |      activations\".\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from transformers.modeling_utils.PreTrainedModel:\n",
      " |  \n",
      " |  is_parallelizable = False\n",
      " |  \n",
      " |  main_input_name = 'input_ids'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __call__ = _call_impl(self, *input, **kwargs)\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      Default dir() implementation.\n",
      " |  \n",
      " |  __getattr__(self, name: str) -> Union[torch.Tensor, ForwardRef('Module')]\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      " |      Adds a child module to the current module.\n",
      " |      \n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the child module. The child module can be\n",
      " |              accessed from this module using the given name\n",
      " |          module (Module): child module to be added to the module.\n",
      " |  \n",
      " |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      " |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      " |      as well as self. Typical use includes initializing the parameters of a model\n",
      " |      (see also :ref:`nn-init-doc`).\n",
      " |      \n",
      " |      Args:\n",
      " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> @torch.no_grad()\n",
      " |          >>> def init_weights(m):\n",
      " |          >>>     print(m)\n",
      " |          >>>     if type(m) == nn.Linear:\n",
      " |          >>>         m.weight.fill_(1.0)\n",
      " |          >>>         print(m.weight)\n",
      " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      " |          >>> net.apply(init_weights)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 1.,  1.],\n",
      " |                  [ 1.,  1.]])\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 1.,  1.],\n",
      " |                  [ 1.,  1.]])\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |  \n",
      " |  bfloat16(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      " |      Returns an iterator over module buffers.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          torch.Tensor: module buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for buf in model.buffers():\n",
      " |          >>>     print(type(buf), buf.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  children(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Returns an iterator over immediate children modules.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a child module\n",
      " |  \n",
      " |  cpu(self: ~T) -> ~T\n",
      " |      Moves all model parameters and buffers to the CPU.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Moves all model parameters and buffers to the GPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on GPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  double(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  eval(self: ~T) -> ~T\n",
      " |      Sets the module in evaluation mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      " |      \n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  extra_repr(self) -> str\n",
      " |      Set the extra representation of the module\n",
      " |      \n",
      " |      To print customized extra information, you should re-implement\n",
      " |      this method in your own modules. Both single-line and multi-line\n",
      " |      strings are acceptable.\n",
      " |  \n",
      " |  float(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  get_buffer(self, target: str) -> 'Tensor'\n",
      " |      Returns the buffer given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the buffer\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.Tensor: The buffer referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not a\n",
      " |              buffer\n",
      " |  \n",
      " |  get_extra_state(self) -> Any\n",
      " |      Returns any extra state to include in the module's state_dict.\n",
      " |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      " |      if you need to store extra state. This function is called when building the\n",
      " |      module's `state_dict()`.\n",
      " |      \n",
      " |      Note that extra state should be pickleable to ensure working serialization\n",
      " |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      " |      for serializing Tensors; other objects may break backwards compatibility if\n",
      " |      their serialized pickled form changes.\n",
      " |      \n",
      " |      Returns:\n",
      " |          object: Any extra state to store in the module's state_dict\n",
      " |  \n",
      " |  get_parameter(self, target: str) -> 'Parameter'\n",
      " |      Returns the parameter given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the Parameter\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Parameter``\n",
      " |  \n",
      " |  get_submodule(self, target: str) -> 'Module'\n",
      " |      Returns the submodule given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      " |      looks like this:\n",
      " |      \n",
      " |      .. code-block:: text\n",
      " |      \n",
      " |          A(\n",
      " |              (net_b): Module(\n",
      " |                  (net_c): Module(\n",
      " |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      " |                  )\n",
      " |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      " |              )\n",
      " |          )\n",
      " |      \n",
      " |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      " |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      " |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      " |      \n",
      " |      To check whether or not we have the ``linear`` submodule, we\n",
      " |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      " |      we have the ``conv`` submodule, we would call\n",
      " |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      " |      \n",
      " |      The runtime of ``get_submodule`` is bounded by the degree\n",
      " |      of module nesting in ``target``. A query against\n",
      " |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      " |      the number of transitive modules. So, for a simple check to see\n",
      " |      if some submodule exists, ``get_submodule`` should always be\n",
      " |      used.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the submodule\n",
      " |              to look for. (See above example for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.nn.Module: The submodule referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Module``\n",
      " |  \n",
      " |  half(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Moves all model parameters and buffers to the IPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on IPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True)\n",
      " |      Copies parameters and buffers from :attr:`state_dict` into\n",
      " |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      " |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      " |      \n",
      " |      Args:\n",
      " |          state_dict (dict): a dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |          strict (bool, optional): whether to strictly enforce that the keys\n",
      " |              in :attr:`state_dict` match the keys returned by this module's\n",
      " |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      " |      \n",
      " |      Returns:\n",
      " |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      " |              * **missing_keys** is a list of str containing the missing keys\n",
      " |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      " |      \n",
      " |      Note:\n",
      " |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      " |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      " |          ``RuntimeError``.\n",
      " |  \n",
      " |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Returns an iterator over all modules in the network.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a module in the network\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |                  print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      " |  \n",
      " |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      " |      Returns an iterator over module buffers, yielding both the\n",
      " |      name of the buffer as well as the buffer itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all buffer names.\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, buf in self.named_buffers():\n",
      " |          >>>    if name in ['running_var']:\n",
      " |          >>>        print(buf.size())\n",
      " |  \n",
      " |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      " |      Returns an iterator over immediate children modules, yielding both\n",
      " |      the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple containing a name and child module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |  \n",
      " |  named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      " |      Returns an iterator over all modules in the network, yielding\n",
      " |      both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          memo: a memo to store the set of modules already added to the result\n",
      " |          prefix: a prefix that will be added to the name of the module\n",
      " |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      " |              or not\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple of name and module\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |                  print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> ('', Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      " |  \n",
      " |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      " |      Returns an iterator over module parameters, yielding both the\n",
      " |      name of the parameter as well as the parameter itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all parameter names.\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Parameter): Tuple containing the name and parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>    if name in ['bias']:\n",
      " |          >>>        print(param.size())\n",
      " |  \n",
      " |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      " |      Returns an iterator over module parameters.\n",
      " |      \n",
      " |      This is typically passed to an optimizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Parameter: module parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param), param.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Optional[torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      " |      the behavior of this function will change in future versions.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None\n",
      " |      Adds a buffer to the module.\n",
      " |      \n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the module's state. Buffers, by\n",
      " |      default, are persistent and will be saved alongside parameters. This\n",
      " |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      " |      only difference between a persistent buffer and a non-persistent buffer\n",
      " |      is that the latter will not be a part of this module's\n",
      " |      :attr:`state_dict`.\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the buffer. The buffer can be accessed\n",
      " |              from this module using the given name\n",
      " |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      " |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      " |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      " |          persistent (bool): whether the buffer is part of this module's\n",
      " |              :attr:`state_dict`.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |  \n",
      " |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a forward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time after :func:`forward` has computed an output.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input, output) -> None or modified output\n",
      " |      \n",
      " |      The input contains only the positional arguments given to the module.\n",
      " |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      " |      The hook can modify the output. It can modify the input inplace but\n",
      " |      it will not have effect on forward since this is called after\n",
      " |      :func:`forward` is called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a forward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time before :func:`forward` is invoked.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input) -> None or modified input\n",
      " |      \n",
      " |      The input contains only the positional arguments given to the module.\n",
      " |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      " |      The hook can modify the input. User can either return a tuple or a\n",
      " |      single modified value in the hook. We will wrap the value into a tuple\n",
      " |      if a single value is returned(unless that value is already a tuple).\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Optional[torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients with respect to module\n",
      " |      inputs are computed. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      " |      \n",
      " |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      " |      with respect to the inputs and outputs respectively. The hook should\n",
      " |      not modify its arguments, but it can optionally return a new gradient with\n",
      " |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      " |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      " |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      " |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      " |      arguments.\n",
      " |      \n",
      " |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      " |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      " |      of each Tensor returned by the Module's forward function.\n",
      " |      \n",
      " |      .. warning ::\n",
      " |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      " |          will raise an error.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_load_state_dict_post_hook(self, hook)\n",
      " |      Registers a post hook to be run after module's ``load_state_dict``\n",
      " |      is called.\n",
      " |      \n",
      " |      It should have the following signature::\n",
      " |          hook(module, incompatible_keys) -> None\n",
      " |      \n",
      " |      The ``module`` argument is the current module that this hook is registered\n",
      " |      on, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\n",
      " |      of attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\n",
      " |      is a ``list`` of ``str`` containing the missing keys and\n",
      " |      ``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n",
      " |      \n",
      " |      The given incompatible_keys can be modified inplace if needed.\n",
      " |      \n",
      " |      Note that the checks performed when calling :func:`load_state_dict` with\n",
      " |      ``strict=True`` are affected by modifications the hook makes to\n",
      " |      ``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\n",
      " |      set of keys will result in an error being thrown when ``strict=True``, and\n",
      " |      clearning out both missing and unexpected keys will avoid an error.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      " |      Alias for :func:`add_module`.\n",
      " |  \n",
      " |  register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None\n",
      " |      Adds a parameter to the module.\n",
      " |      \n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the parameter. The parameter can be accessed\n",
      " |              from this module using the given name\n",
      " |          param (Parameter or None): parameter to be added to the module. If\n",
      " |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      " |              are ignored. If ``None``, the parameter is **not** included in the\n",
      " |              module's :attr:`state_dict`.\n",
      " |  \n",
      " |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      " |      Change if autograd should record operations on parameters in this\n",
      " |      module.\n",
      " |      \n",
      " |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      " |      in-place.\n",
      " |      \n",
      " |      This method is helpful for freezing part of the module for finetuning\n",
      " |      or training parts of a model individually (e.g., GAN training).\n",
      " |      \n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      " |      \n",
      " |      Args:\n",
      " |          requires_grad (bool): whether autograd should record operations on\n",
      " |                                parameters in this module. Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  set_extra_state(self, state: Any)\n",
      " |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      " |      found within the `state_dict`. Implement this function and a corresponding\n",
      " |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      " |      `state_dict`.\n",
      " |      \n",
      " |      Args:\n",
      " |          state (dict): Extra state from the `state_dict`\n",
      " |  \n",
      " |  share_memory(self: ~T) -> ~T\n",
      " |      See :meth:`torch.Tensor.share_memory_`\n",
      " |  \n",
      " |  state_dict(self, *args, destination=None, prefix='', keep_vars=False)\n",
      " |      Returns a dictionary containing a whole state of the module.\n",
      " |      \n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      Parameters and buffers set to ``None`` are not included.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          Currently ``state_dict()`` also accepts positional arguments for\n",
      " |          ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n",
      " |          this is being deprecated and keyword arguments will be enforced in\n",
      " |          future releases.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          Please avoid the use of argument ``destination`` as it is not\n",
      " |          designed for end-users.\n",
      " |      \n",
      " |      Args:\n",
      " |          destination (dict, optional): If provided, the state of module will\n",
      " |              be updated into the dict and the same object is returned.\n",
      " |              Otherwise, an ``OrderedDict`` will be created and returned.\n",
      " |              Default: ``None``.\n",
      " |          prefix (str, optional): a prefix added to parameter and buffer\n",
      " |              names to compose the keys in state_dict. Default: ``''``.\n",
      " |          keep_vars (bool, optional): by default the :class:`~torch.Tensor` s\n",
      " |              returned in the state dict are detached from autograd. If it's\n",
      " |              set to ``True``, detaching will not be performed.\n",
      " |              Default: ``False``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict:\n",
      " |              a dictionary containing a whole state of the module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |  \n",
      " |  to(self, *args, **kwargs)\n",
      " |      Moves and/or casts the parameters and buffers.\n",
      " |      \n",
      " |      This can be called as\n",
      " |      \n",
      " |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(dtype, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(tensor, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(memory_format=torch.channels_last)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      " |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      " |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      " |      (if given). The integral parameters and buffers will be moved\n",
      " |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      " |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      " |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      " |      pinned memory to CUDA devices.\n",
      " |      \n",
      " |      See below for examples.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): the desired device of the parameters\n",
      " |              and buffers in this module\n",
      " |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      " |              the parameters and buffers in this module\n",
      " |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      " |              dtype and device for all parameters and buffers in this module\n",
      " |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      " |              format for 4D parameters and buffers in this module (keyword\n",
      " |              only argument)\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          >>> linear = nn.Linear(2, 2)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]])\n",
      " |          >>> linear.to(torch.double)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      " |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      " |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      " |          >>> cpu = torch.device(\"cpu\")\n",
      " |          >>> linear.to(cpu)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      " |      \n",
      " |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      " |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      " |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      " |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      " |  \n",
      " |  to_empty(self: ~T, *, device: Union[str, torch.device]) -> ~T\n",
      " |      Moves the parameters and buffers to the specified device without copying storage.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): The desired device of the parameters\n",
      " |              and buffers in this module.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  train(self: ~T, mode: bool = True) -> ~T\n",
      " |      Sets the module in training mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      " |                       mode (``False``). Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      " |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          dst_type (type or string): the desired type\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Moves all model parameters and buffers to the XPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on XPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  zero_grad(self, set_to_none: bool = False) -> None\n",
      " |      Sets gradients of all model parameters to zero. See similar function\n",
      " |      under :class:`torch.optim.Optimizer` for more context.\n",
      " |      \n",
      " |      Args:\n",
      " |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      " |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  T_destination = ~T_destination\n",
      " |  \n",
      " |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      " |  \n",
      " |  dump_patches = False\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.modeling_utils.ModuleUtilsMixin:\n",
      " |  \n",
      " |  add_memory_hooks(self)\n",
      " |      Add a memory hook before and after each sub-module forward pass to record increase in memory consumption.\n",
      " |      \n",
      " |      Increase in memory consumption is stored in a `mem_rss_diff` attribute for each module and can be reset to zero\n",
      " |      with `model.reset_memory_hooks_state()`.\n",
      " |  \n",
      " |  estimate_tokens(self, input_dict: Dict[str, Union[torch.Tensor, Any]]) -> int\n",
      " |      Helper function to estimate the total number of tokens from the model inputs.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs (`dict`): The model inputs.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `int`: The total number of tokens.\n",
      " |  \n",
      " |  floating_point_ops(self, input_dict: Dict[str, Union[torch.Tensor, Any]], exclude_embeddings: bool = True) -> int\n",
      " |      Get number of (optionally, non-embeddings) floating-point operations for the forward and backward passes of a\n",
      " |      batch with this transformer model. Default approximation neglects the quadratic dependency on the number of\n",
      " |      tokens (valid if `12 * d_model << sequence_length`) as laid out in [this\n",
      " |      paper](https://arxiv.org/pdf/2001.08361.pdf) section 2.1. Should be overridden for transformers with parameter\n",
      " |      re-use e.g. Albert or Universal Transformers, or if doing long-range modeling with very high sequence lengths.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch_size (`int`):\n",
      " |              The batch size for the forward pass.\n",
      " |      \n",
      " |          sequence_length (`int`):\n",
      " |              The number of tokens in each line of the batch.\n",
      " |      \n",
      " |          exclude_embeddings (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to count embedding and softmax operations.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `int`: The number of floating-point operations.\n",
      " |  \n",
      " |  get_extended_attention_mask(self, attention_mask: torch.Tensor, input_shape: Tuple[int], device: <property object at 0x7fee332fa630> = None, dtype: torch.float32 = None) -> torch.Tensor\n",
      " |      Makes broadcastable attention and causal masks so that future and masked tokens are ignored.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          attention_mask (`torch.Tensor`):\n",
      " |              Mask with ones indicating tokens to attend to, zeros for tokens to ignore.\n",
      " |          input_shape (`Tuple[int]`):\n",
      " |              The shape of the input to the model.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `torch.Tensor` The extended attention mask, with a the same dtype as `attention_mask.dtype`.\n",
      " |  \n",
      " |  get_head_mask(self, head_mask: Optional[torch.Tensor], num_hidden_layers: int, is_attention_chunked: bool = False) -> torch.Tensor\n",
      " |      Prepare the head mask if needed.\n",
      " |      \n",
      " |      Args:\n",
      " |          head_mask (`torch.Tensor` with shape `[num_heads]` or `[num_hidden_layers x num_heads]`, *optional*):\n",
      " |              The mask indicating if we should keep the heads or not (1.0 for keep, 0.0 for discard).\n",
      " |          num_hidden_layers (`int`):\n",
      " |              The number of hidden layers in the model.\n",
      " |          is_attention_chunked: (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not the attentions scores are computed by chunks or not.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `torch.Tensor` with shape `[num_hidden_layers x batch x num_heads x seq_length x seq_length]` or list with\n",
      " |          `[None]` for each layer.\n",
      " |  \n",
      " |  invert_attention_mask(self, encoder_attention_mask: torch.Tensor) -> torch.Tensor\n",
      " |      Invert an attention mask (e.g., switches 0. and 1.).\n",
      " |      \n",
      " |      Args:\n",
      " |          encoder_attention_mask (`torch.Tensor`): An attention mask.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `torch.Tensor`: The inverted attention mask.\n",
      " |  \n",
      " |  num_parameters(self, only_trainable: bool = False, exclude_embeddings: bool = False) -> int\n",
      " |      Get number of (optionally, trainable or non-embeddings) parameters in the module.\n",
      " |      \n",
      " |      Args:\n",
      " |          only_trainable (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return only the number of trainable parameters\n",
      " |      \n",
      " |          exclude_embeddings (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return only the number of non-embeddings parameters\n",
      " |      \n",
      " |      Returns:\n",
      " |          `int`: The number of parameters.\n",
      " |  \n",
      " |  reset_memory_hooks_state(self)\n",
      " |      Reset the `mem_rss_diff` attribute of each module (see [`~modeling_utils.ModuleUtilsMixin.add_memory_hooks`]).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from transformers.modeling_utils.ModuleUtilsMixin:\n",
      " |  \n",
      " |  create_extended_attention_mask_for_decoder(input_shape, attention_mask, device=None)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from transformers.modeling_utils.ModuleUtilsMixin:\n",
      " |  \n",
      " |  device\n",
      " |      `torch.device`: The device on which the module is (assuming that all the module parameters are on the same\n",
      " |      device).\n",
      " |  \n",
      " |  dtype\n",
      " |      `torch.dtype`: The dtype of the module (assuming that all the module parameters have the same dtype).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.generation_utils.GenerationMixin:\n",
      " |  \n",
      " |  adjust_logits_during_generation(self, logits: torch.FloatTensor, **kwargs) -> torch.FloatTensor\n",
      " |      Implement in subclasses of [`PreTrainedModel`] for custom behavior to adjust the logits in the generate method.\n",
      " |  \n",
      " |  beam_sample(self, input_ids: torch.LongTensor, beam_scorer: transformers.generation_beam_search.BeamScorer, logits_processor: Optional[transformers.generation_logits_process.LogitsProcessorList] = None, stopping_criteria: Optional[transformers.generation_stopping_criteria.StoppingCriteriaList] = None, logits_warper: Optional[transformers.generation_logits_process.LogitsProcessorList] = None, max_length: Optional[int] = None, pad_token_id: Optional[int] = None, eos_token_id: Optional[int] = None, output_attentions: Optional[bool] = None, output_hidden_states: Optional[bool] = None, output_scores: Optional[bool] = None, return_dict_in_generate: Optional[bool] = None, synced_gpus: Optional[bool] = False, **model_kwargs) -> Union[transformers.generation_utils.BeamSampleEncoderDecoderOutput, transformers.generation_utils.BeamSampleDecoderOnlyOutput, torch.LongTensor]\n",
      " |      Generates sequences of token ids for models with a language modeling head using **beam search multinomial\n",
      " |      sampling** and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      " |              The sequence used as a prompt for the generation.\n",
      " |          beam_scorer (`BeamScorer`):\n",
      " |              A derived instance of [`BeamScorer`] that defines how beam hypotheses are constructed, stored and\n",
      " |              sorted during generation. For more information, the documentation of [`BeamScorer`] should be read.\n",
      " |          logits_processor (`LogitsProcessorList`, *optional*):\n",
      " |              An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsProcessor`]\n",
      " |              used to modify the prediction scores of the language modeling head applied at each generation step.\n",
      " |          stopping_criteria (`StoppingCriteriaList`, *optional*):\n",
      " |              An instance of [`StoppingCriteriaList`]. List of instances of class derived from [`StoppingCriteria`]\n",
      " |              used to tell if the generation loop should stop.\n",
      " |          logits_warper (`LogitsProcessorList`, *optional*):\n",
      " |              An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsWarper`] used\n",
      " |              to warp the prediction score distribution of the language modeling head applied before multinomial\n",
      " |              sampling at each generation step.\n",
      " |          max_length (`int`, *optional*, defaults to 20):\n",
      " |              **DEPRECATED**. Use `logits_processor` or `stopping_criteria` directly to cap the number of generated\n",
      " |              tokens. The maximum length of the sequence to be generated.\n",
      " |          pad_token_id (`int`, *optional*):\n",
      " |              The id of the *padding* token.\n",
      " |          eos_token_id (`int`, *optional*):\n",
      " |              The id of the *end-of-sequence* token.\n",
      " |          output_attentions (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
      " |              returned tensors for more details.\n",
      " |          output_hidden_states (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
      " |              for more details.\n",
      " |          output_scores (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the prediction scores. See `scores` under returned tensors for more details.\n",
      " |          return_dict_in_generate (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
      " |          synced_gpus (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n",
      " |          model_kwargs:\n",
      " |              Additional model specific kwargs will be forwarded to the `forward` function of the model. If model is\n",
      " |              an encoder-decoder model the kwargs should include `encoder_outputs`.\n",
      " |      \n",
      " |      Return:\n",
      " |          [`~generation_utils.BeamSampleDecoderOnlyOutput`], [`~generation_utils.BeamSampleEncoderDecoderOutput`] or\n",
      " |          `torch.LongTensor`: A `torch.LongTensor` containing the generated tokens (default behaviour) or a\n",
      " |          [`~generation_utils.BeamSampleDecoderOnlyOutput`] if `model.config.is_encoder_decoder=False` and\n",
      " |          `return_dict_in_generate=True` or a [`~generation_utils.BeamSampleEncoderDecoderOutput`] if\n",
      " |          `model.config.is_encoder_decoder=True`.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      >>> from transformers import (\n",
      " |      ...     AutoTokenizer,\n",
      " |      ...     AutoModelForSeq2SeqLM,\n",
      " |      ...     LogitsProcessorList,\n",
      " |      ...     MinLengthLogitsProcessor,\n",
      " |      ...     TopKLogitsWarper,\n",
      " |      ...     TemperatureLogitsWarper,\n",
      " |      ...     BeamSearchScorer,\n",
      " |      ... )\n",
      " |      >>> import torch\n",
      " |      \n",
      " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
      " |      >>> model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
      " |      \n",
      " |      >>> encoder_input_str = \"translate English to German: How old are you?\"\n",
      " |      >>> encoder_input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids\n",
      " |      \n",
      " |      >>> # lets run beam search using 3 beams\n",
      " |      >>> num_beams = 3\n",
      " |      >>> # define decoder start token ids\n",
      " |      >>> input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)\n",
      " |      >>> input_ids = input_ids * model.config.decoder_start_token_id\n",
      " |      \n",
      " |      >>> # add encoder_outputs to model keyword arguments\n",
      " |      >>> model_kwargs = {\n",
      " |      ...     \"encoder_outputs\": model.get_encoder()(\n",
      " |      ...         encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True\n",
      " |      ...     )\n",
      " |      ... }\n",
      " |      \n",
      " |      >>> # instantiate beam scorer\n",
      " |      >>> beam_scorer = BeamSearchScorer(\n",
      " |      ...     batch_size=1,\n",
      " |      ...     max_length=model.config.max_length,\n",
      " |      ...     num_beams=num_beams,\n",
      " |      ...     device=model.device,\n",
      " |      ... )\n",
      " |      \n",
      " |      >>> # instantiate logits processors\n",
      " |      >>> logits_processor = LogitsProcessorList(\n",
      " |      ...     [MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id)]\n",
      " |      ... )\n",
      " |      >>> # instantiate logits processors\n",
      " |      >>> logits_warper = LogitsProcessorList(\n",
      " |      ...     [\n",
      " |      ...         TopKLogitsWarper(50),\n",
      " |      ...         TemperatureLogitsWarper(0.7),\n",
      " |      ...     ]\n",
      " |      ... )\n",
      " |      \n",
      " |      >>> outputs = model.beam_sample(\n",
      " |      ...     input_ids, beam_scorer, logits_processor=logits_processor, logits_warper=logits_warper, **model_kwargs\n",
      " |      ... )\n",
      " |      \n",
      " |      >>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
      " |      ['Wie alt bist du?']\n",
      " |      ```\n",
      " |  \n",
      " |  beam_search(self, input_ids: torch.LongTensor, beam_scorer: transformers.generation_beam_search.BeamScorer, logits_processor: Optional[transformers.generation_logits_process.LogitsProcessorList] = None, stopping_criteria: Optional[transformers.generation_stopping_criteria.StoppingCriteriaList] = None, max_length: Optional[int] = None, pad_token_id: Optional[int] = None, eos_token_id: Optional[int] = None, output_attentions: Optional[bool] = None, output_hidden_states: Optional[bool] = None, output_scores: Optional[bool] = None, return_dict_in_generate: Optional[bool] = None, synced_gpus: Optional[bool] = False, **model_kwargs) -> Union[transformers.generation_utils.BeamSearchEncoderDecoderOutput, transformers.generation_utils.BeamSearchDecoderOnlyOutput, torch.LongTensor]\n",
      " |      Generates sequences of token ids for models with a language modeling head using **beam search decoding** and\n",
      " |      can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      " |              The sequence used as a prompt for the generation.\n",
      " |          beam_scorer (`BeamScorer`):\n",
      " |              An derived instance of [`BeamScorer`] that defines how beam hypotheses are constructed, stored and\n",
      " |              sorted during generation. For more information, the documentation of [`BeamScorer`] should be read.\n",
      " |          logits_processor (`LogitsProcessorList`, *optional*):\n",
      " |              An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsProcessor`]\n",
      " |              used to modify the prediction scores of the language modeling head applied at each generation step.\n",
      " |          stopping_criteria (`StoppingCriteriaList`, *optional*):\n",
      " |              An instance of [`StoppingCriteriaList`]. List of instances of class derived from [`StoppingCriteria`]\n",
      " |              used to tell if the generation loop should stop.\n",
      " |          max_length (`int`, *optional*, defaults to 20):\n",
      " |              **DEPRECATED**. Use `logits_processor` or `stopping_criteria` directly to cap the number of generated\n",
      " |              tokens. The maximum length of the sequence to be generated.\n",
      " |          pad_token_id (`int`, *optional*):\n",
      " |              The id of the *padding* token.\n",
      " |          eos_token_id (`int`, *optional*):\n",
      " |              The id of the *end-of-sequence* token.\n",
      " |          output_attentions (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
      " |              returned tensors for more details.\n",
      " |          output_hidden_states (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
      " |              for more details.\n",
      " |          output_scores (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the prediction scores. See `scores` under returned tensors for more details.\n",
      " |          return_dict_in_generate (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
      " |          synced_gpus (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n",
      " |          model_kwargs:\n",
      " |              Additional model specific kwargs will be forwarded to the `forward` function of the model. If model is\n",
      " |              an encoder-decoder model the kwargs should include `encoder_outputs`.\n",
      " |      \n",
      " |      Return:\n",
      " |          [`generation_utilsBeamSearchDecoderOnlyOutput`], [`~generation_utils.BeamSearchEncoderDecoderOutput`] or\n",
      " |          `torch.LongTensor`: A `torch.LongTensor` containing the generated tokens (default behaviour) or a\n",
      " |          [`~generation_utils.BeamSearchDecoderOnlyOutput`] if `model.config.is_encoder_decoder=False` and\n",
      " |          `return_dict_in_generate=True` or a [`~generation_utils.BeamSearchEncoderDecoderOutput`] if\n",
      " |          `model.config.is_encoder_decoder=True`.\n",
      " |      \n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      >>> from transformers import (\n",
      " |      ...     AutoTokenizer,\n",
      " |      ...     AutoModelForSeq2SeqLM,\n",
      " |      ...     LogitsProcessorList,\n",
      " |      ...     MinLengthLogitsProcessor,\n",
      " |      ...     BeamSearchScorer,\n",
      " |      ... )\n",
      " |      >>> import torch\n",
      " |      \n",
      " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
      " |      >>> model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
      " |      \n",
      " |      >>> encoder_input_str = \"translate English to German: How old are you?\"\n",
      " |      >>> encoder_input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids\n",
      " |      \n",
      " |      \n",
      " |      >>> # lets run beam search using 3 beams\n",
      " |      >>> num_beams = 3\n",
      " |      >>> # define decoder start token ids\n",
      " |      >>> input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)\n",
      " |      >>> input_ids = input_ids * model.config.decoder_start_token_id\n",
      " |      \n",
      " |      >>> # add encoder_outputs to model keyword arguments\n",
      " |      >>> model_kwargs = {\n",
      " |      ...     \"encoder_outputs\": model.get_encoder()(\n",
      " |      ...         encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True\n",
      " |      ...     )\n",
      " |      ... }\n",
      " |      \n",
      " |      >>> # instantiate beam scorer\n",
      " |      >>> beam_scorer = BeamSearchScorer(\n",
      " |      ...     batch_size=1,\n",
      " |      ...     num_beams=num_beams,\n",
      " |      ...     device=model.device,\n",
      " |      ... )\n",
      " |      \n",
      " |      >>> # instantiate logits processors\n",
      " |      >>> logits_processor = LogitsProcessorList(\n",
      " |      ...     [\n",
      " |      ...         MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),\n",
      " |      ...     ]\n",
      " |      ... )\n",
      " |      \n",
      " |      >>> outputs = model.beam_search(input_ids, beam_scorer, logits_processor=logits_processor, **model_kwargs)\n",
      " |      \n",
      " |      >>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
      " |      ['Wie alt bist du?']\n",
      " |      ```\n",
      " |  \n",
      " |  compute_transition_beam_scores(self, sequences: torch.Tensor, scores: Tuple[torch.Tensor], beam_indices: torch.Tensor, eos_token_id: int = None)\n",
      " |      compute the transition probabilities of sequences given generation\n",
      " |      scores and beam indices\n",
      " |  \n",
      " |  constrained_beam_search(self, input_ids: torch.LongTensor, constrained_beam_scorer: transformers.generation_beam_search.ConstrainedBeamSearchScorer, logits_processor: Optional[transformers.generation_logits_process.LogitsProcessorList] = None, stopping_criteria: Optional[transformers.generation_stopping_criteria.StoppingCriteriaList] = None, max_length: Optional[int] = None, pad_token_id: Optional[int] = None, eos_token_id: Optional[int] = None, output_attentions: Optional[bool] = None, output_hidden_states: Optional[bool] = None, output_scores: Optional[bool] = None, return_dict_in_generate: Optional[bool] = None, synced_gpus: Optional[bool] = None, **model_kwargs) -> Union[transformers.generation_utils.BeamSearchEncoderDecoderOutput, transformers.generation_utils.BeamSearchDecoderOnlyOutput, torch.LongTensor]\n",
      " |      Generates sequences of token ids for models with a language modeling head using **constrained beam search\n",
      " |      decoding** and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      " |              The sequence used as a prompt for the generation.\n",
      " |          constrained_beam_scorer (`ConstrainedBeamSearchScorer`):\n",
      " |              A derived instance of [`BeamScorer`] that defines how beam hypotheses are constructed, stored and\n",
      " |              sorted during generation, while satisfying a list of positive constraints. For more information, the\n",
      " |              documentation of [`ConstrainedBeamSearchScorer`] should be read.\n",
      " |          logits_processor (`LogitsProcessorList`, *optional*):\n",
      " |              An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsProcessor`]\n",
      " |              used to modify the prediction scores of the language modeling head applied at each generation step.\n",
      " |          stopping_criteria (`StoppingCriteriaList`, *optional*):\n",
      " |              An instance of [`StoppingCriteriaList`]. List of instances of class derived from [`StoppingCriteria`]\n",
      " |              used to tell if the generation loop should stop.\n",
      " |          logits_warper (`LogitsProcessorList`, *optional*):\n",
      " |              An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsWarper`] used\n",
      " |              to warp the prediction score distribution of the language modeling head applied before multinomial\n",
      " |              sampling at each generation step.\n",
      " |          max_length (`int`, *optional*, defaults to 20):\n",
      " |              **DEPRECATED**. Use `logits_processor` or `stopping_criteria` directly to cap the number of generated\n",
      " |              tokens. The maximum length of the sequence to be generated.\n",
      " |          pad_token_id (`int`, *optional*):\n",
      " |              The id of the *padding* token.\n",
      " |          eos_token_id (`int`, *optional*):\n",
      " |              The id of the *end-of-sequence* token.\n",
      " |          output_attentions (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
      " |              returned tensors for more details.\n",
      " |          output_hidden_states (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
      " |              for more details.\n",
      " |          output_scores (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the prediction scores. See `scores` under returned tensors for more details.\n",
      " |          return_dict_in_generate (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
      " |          synced_gpus (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n",
      " |          model_kwargs:\n",
      " |              Additional model specific kwargs will be forwarded to the `forward` function of the model. If model is\n",
      " |              an encoder-decoder model the kwargs should include `encoder_outputs`.\n",
      " |      \n",
      " |      Return:\n",
      " |          [`generation_utilsBeamSearchDecoderOnlyOutput`], [`~generation_utils.BeamSearchEncoderDecoderOutput`] or\n",
      " |          `torch.LongTensor`: A `torch.LongTensor` containing the generated tokens (default behaviour) or a\n",
      " |          [`~generation_utils.BeamSearchDecoderOnlyOutput`] if `model.config.is_encoder_decoder=False` and\n",
      " |          `return_dict_in_generate=True` or a [`~generation_utils.BeamSearchEncoderDecoderOutput`] if\n",
      " |          `model.config.is_encoder_decoder=True`.\n",
      " |      \n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      >>> from transformers import (\n",
      " |      ...     AutoTokenizer,\n",
      " |      ...     AutoModelForSeq2SeqLM,\n",
      " |      ...     LogitsProcessorList,\n",
      " |      ...     MinLengthLogitsProcessor,\n",
      " |      ...     ConstrainedBeamSearchScorer,\n",
      " |      ...     PhrasalConstraint,\n",
      " |      ... )\n",
      " |      >>> import torch\n",
      " |      \n",
      " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
      " |      >>> model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
      " |      \n",
      " |      >>> encoder_input_str = \"translate English to German: How old are you?\"\n",
      " |      >>> encoder_input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids\n",
      " |      \n",
      " |      \n",
      " |      >>> # lets run beam search using 3 beams\n",
      " |      >>> num_beams = 3\n",
      " |      >>> # define decoder start token ids\n",
      " |      >>> input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)\n",
      " |      >>> input_ids = input_ids * model.config.decoder_start_token_id\n",
      " |      \n",
      " |      >>> # add encoder_outputs to model keyword arguments\n",
      " |      >>> model_kwargs = {\n",
      " |      ...     \"encoder_outputs\": model.get_encoder()(\n",
      " |      ...         encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True\n",
      " |      ...     )\n",
      " |      ... }\n",
      " |      \n",
      " |      >>> constraint_str = \"Sie\"\n",
      " |      >>> constraint_token_ids = tokenizer.encode(constraint_str)[:-1]  # slice to remove eos token\n",
      " |      >>> constraints = [PhrasalConstraint(token_ids=constraint_token_ids)]\n",
      " |      \n",
      " |      \n",
      " |      >>> # instantiate beam scorer\n",
      " |      >>> beam_scorer = ConstrainedBeamSearchScorer(\n",
      " |      ...     batch_size=1, num_beams=num_beams, device=model.device, constraints=constraints\n",
      " |      ... )\n",
      " |      \n",
      " |      >>> # instantiate logits processors\n",
      " |      >>> logits_processor = LogitsProcessorList(\n",
      " |      ...     [\n",
      " |      ...         MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),\n",
      " |      ...     ]\n",
      " |      ... )\n",
      " |      \n",
      " |      >>> outputs = model.constrained_beam_search(\n",
      " |      ...     input_ids, beam_scorer, constraints=constraints, logits_processor=logits_processor, **model_kwargs\n",
      " |      ... )\n",
      " |      \n",
      " |      >>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
      " |      ['Wie alt sind Sie?']\n",
      " |      ```\n",
      " |  \n",
      " |  generate(self, inputs: Optional[torch.Tensor] = None, max_length: Optional[int] = None, min_length: Optional[int] = None, do_sample: Optional[bool] = None, early_stopping: Optional[bool] = None, num_beams: Optional[int] = None, temperature: Optional[float] = None, top_k: Optional[int] = None, top_p: Optional[float] = None, typical_p: Optional[float] = None, repetition_penalty: Optional[float] = None, bad_words_ids: Optional[Iterable[int]] = None, force_words_ids: Union[Iterable[int], Iterable[Iterable[int]], NoneType] = None, bos_token_id: Optional[int] = None, pad_token_id: Optional[int] = None, eos_token_id: Optional[int] = None, length_penalty: Optional[float] = None, no_repeat_ngram_size: Optional[int] = None, encoder_no_repeat_ngram_size: Optional[int] = None, num_return_sequences: Optional[int] = None, max_time: Optional[float] = None, max_new_tokens: Optional[int] = None, decoder_start_token_id: Optional[int] = None, use_cache: Optional[bool] = None, num_beam_groups: Optional[int] = None, diversity_penalty: Optional[float] = None, prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None, logits_processor: Optional[transformers.generation_logits_process.LogitsProcessorList] = [], renormalize_logits: Optional[bool] = None, stopping_criteria: Optional[transformers.generation_stopping_criteria.StoppingCriteriaList] = [], constraints: Optional[List[transformers.generation_beam_constraints.Constraint]] = None, output_attentions: Optional[bool] = None, output_hidden_states: Optional[bool] = None, output_scores: Optional[bool] = None, return_dict_in_generate: Optional[bool] = None, forced_bos_token_id: Optional[int] = None, forced_eos_token_id: Optional[int] = None, remove_invalid_values: Optional[bool] = None, synced_gpus: Optional[bool] = False, exponential_decay_length_penalty: Optional[Tuple[Union[int, float]]] = None, **model_kwargs) -> Union[transformers.generation_utils.GreedySearchEncoderDecoderOutput, transformers.generation_utils.GreedySearchDecoderOnlyOutput, transformers.generation_utils.SampleEncoderDecoderOutput, transformers.generation_utils.SampleDecoderOnlyOutput, transformers.generation_utils.BeamSearchEncoderDecoderOutput, transformers.generation_utils.BeamSearchDecoderOnlyOutput, transformers.generation_utils.BeamSampleEncoderDecoderOutput, transformers.generation_utils.BeamSampleDecoderOnlyOutput, torch.LongTensor]\n",
      " |      Generates sequences of token ids for models with a language modeling head. The method supports the following\n",
      " |      generation methods for text-decoder, text-to-text, speech-to-text, and vision-to-text models:\n",
      " |      \n",
      " |          - *greedy decoding* by calling [`~generation_utils.GenerationMixin.greedy_search`] if `num_beams=1` and\n",
      " |            `do_sample=False`.\n",
      " |          - *multinomial sampling* by calling [`~generation_utils.GenerationMixin.sample`] if `num_beams=1` and\n",
      " |            `do_sample=True`.\n",
      " |          - *beam-search decoding* by calling [`~generation_utils.GenerationMixin.beam_search`] if `num_beams>1` and\n",
      " |            `do_sample=False`.\n",
      " |          - *beam-search multinomial sampling* by calling [`~generation_utils.GenerationMixin.beam_sample`] if\n",
      " |            `num_beams>1` and `do_sample=True`.\n",
      " |          - *diverse beam-search decoding* by calling [`~generation_utils.GenerationMixin.group_beam_search`], if\n",
      " |            `num_beams>1` and `num_beam_groups>1`.\n",
      " |          - *constrained beam-search decoding* by calling\n",
      " |            [`~generation_utils.GenerationMixin.constrained_beam_search`], if `constraints!=None` or\n",
      " |            `force_words_ids!=None`.\n",
      " |      \n",
      " |      <Tip warning={true}>\n",
      " |      \n",
      " |      Apart from `inputs`, all the arguments below will default to the value of the attribute of the same name as\n",
      " |      defined in the model's config (`config.json`) which in turn defaults to the\n",
      " |      [`~modeling_utils.PretrainedConfig`] of the model.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Most of these parameters are explained in more detail in [this blog\n",
      " |      post](https://huggingface.co/blog/how-to-generate).\n",
      " |      \n",
      " |      Parameters:\n",
      " |          inputs (`torch.Tensor` of varying shape depending on the modality, *optional*):\n",
      " |              The sequence used as a prompt for the generation or as model inputs to the encoder. If `None` the\n",
      " |              method initializes it with `bos_token_id` and a batch size of 1. For decoder-only models `inputs`\n",
      " |              should of in the format of `input_ids`. For encoder-decoder models *inputs* can represent any of\n",
      " |              `input_ids`, `input_values`, `input_features`, or `pixel_values`.\n",
      " |          max_length (`int`, *optional*, defaults to `model.config.max_length`):\n",
      " |              The maximum length the generated tokens can have. Corresponds to the length of the input prompt +\n",
      " |              `max_new_tokens`. In general, prefer the use of `max_new_tokens`, which ignores the number of tokens in\n",
      " |              the prompt.\n",
      " |          max_new_tokens (`int`, *optional*):\n",
      " |              The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.\n",
      " |          min_length (`int`, *optional*, defaults to `model.config.min_length` or 10 if the config does not set any value):\n",
      " |              The minimum length of the sequence to be generated.\n",
      " |          do_sample (`bool`, *optional*, defaults to `model.config.do_sample` or `False` if the config does not set any value):\n",
      " |              Whether or not to use sampling ; use greedy decoding otherwise.\n",
      " |          early_stopping (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether to stop the beam search when at least `num_beams` sentences are finished per batch or not.\n",
      " |          num_beams (`int`, *optional*, defaults to `model.config.num_beams` or 1 if the config does not set any value):\n",
      " |              Number of beams for beam search. 1 means no beam search.\n",
      " |          temperature (`float`, *optional*, defaults to `model.config.temperature` or 1.0 if the config does not set any value):\n",
      " |              The value used to module the next token probabilities.\n",
      " |          top_k (`int`, *optional*, defaults to `model.config.top_k` or 50 if the config does not set any value):\n",
      " |              The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
      " |          top_p (`float`, *optional*, defaults to `model.config.top_p` or 1.0 if the config does not set any value):\n",
      " |              If set to float < 1, only the most probable tokens with probabilities that add up to `top_p` or higher\n",
      " |              are kept for generation.\n",
      " |          typical_p (`float`, *optional*, defaults to `model.config.typical_p` or 1.0 if the config does not set any value):\n",
      " |              The amount of probability mass from the original distribution to be considered in typical decoding. If\n",
      " |              set to 1.0 it takes no effect. See [this paper](https://arxiv.org/pdf/2202.00666.pdf) for more details.\n",
      " |          repetition_penalty (`float`, *optional*, defaults to `model.config.repetition_penalty` or 1.0 if the config does not set any value):\n",
      " |              The parameter for repetition penalty. 1.0 means no penalty. See [this\n",
      " |              paper](https://arxiv.org/pdf/1909.05858.pdf) for more details.\n",
      " |          pad_token_id (`int`, *optional*, defaults to `model.config.pad_token_id`):\n",
      " |              The id of the *padding* token.\n",
      " |          bos_token_id (`int`, *optional*, defaults to `model.config.bos_token_id`):\n",
      " |              The id of the *beginning-of-sequence* token.\n",
      " |          eos_token_id (`int`, *optional*, defaults to `model.config.eos_token_id`):\n",
      " |              The id of the *end-of-sequence* token.\n",
      " |          length_penalty (`float`, *optional*, defaults to `model.config.length_penalty` or 1.0 if the config does not set any value):\n",
      " |               Exponential penalty to the length. 1.0 means that the beam score is penalized by the sequence length.\n",
      " |               0.0 means no penalty. Set to values < 0.0 in order to encourage the model to generate longer\n",
      " |               sequences, to a value > 0.0 in order to encourage the model to produce shorter sequences.\n",
      " |          no_repeat_ngram_size (`int`, *optional*, defaults to `model.config.no_repeat_ngram_size` or 0 if the config does not set any value):\n",
      " |              If set to int > 0, all ngrams of that size can only occur once.\n",
      " |          encoder_no_repeat_ngram_size (`int`, *optional*, defaults to `model.config.encoder_no_repeat_ngram_size` or 0 if the config does not set any value):\n",
      " |              If set to int > 0, all ngrams of that size that occur in the `encoder_input_ids` cannot occur in the\n",
      " |              `decoder_input_ids`.\n",
      " |          bad_words_ids(`List[List[int]]`, *optional*, defaults to `model.config.bad_words_ids`):\n",
      " |              List of token ids that are not allowed to be generated. In order to get the token ids of the words that\n",
      " |              should not appear in the generated text, use `tokenizer(bad_words, add_prefix_space=True,\n",
      " |              add_special_tokens=False).input_ids`.\n",
      " |          force_words_ids(`List[List[int]]` or `List[List[List[int]]]`, *optional*):\n",
      " |              List of token ids that must be generated. If given a `List[List[int]]`, this is treated as a simple\n",
      " |              list of words that must be included, the opposite to `bad_words_ids`. If given `List[List[List[int]]]`,\n",
      " |              this triggers a [disjunctive constraint](https://github.com/huggingface/transformers/issues/14081),\n",
      " |              where one can allow different forms of each word.\n",
      " |          num_return_sequences(`int`, *optional*, defaults to `model.config.num_return_sequences` or 1 if the config does not set any value):\n",
      " |              The number of independently computed returned sequences for each element in the batch.\n",
      " |          max_time(`float`, *optional*):\n",
      " |              The maximum amount of time you allow the computation to run for in seconds. generation will still\n",
      " |              finish the current pass after allocated time has been passed.\n",
      " |          attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      " |              Mask to avoid performing attention on padding token indices. Mask values are in `[0, 1]`, 1 for tokens\n",
      " |              that are not masked, and 0 for masked tokens. If not provided, will default to a tensor the same shape\n",
      " |              as `input_ids` that masks the pad token. [What are attention masks?](../glossary#attention-mask)\n",
      " |          decoder_start_token_id (`int`, *optional*):\n",
      " |              If an encoder-decoder model starts decoding with a different token than *bos*, the id of that token.\n",
      " |          use_cache: (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not the model should use the past last key/values attentions (if applicable to the model) to\n",
      " |              speed up decoding.\n",
      " |          num_beam_groups (`int`, *optional*, defaults to `model.config.num_beam_groups` or 1 if the config does not set any value):\n",
      " |              Number of groups to divide `num_beams` into in order to ensure diversity among different groups of\n",
      " |              beams. [this paper](https://arxiv.org/pdf/1610.02424.pdf) for more details.\n",
      " |          diversity_penalty (`float`, *optional*, defaults to `model.config.diversity_penalty` or 0.0 if the config does not set any value):\n",
      " |              This value is subtracted from a beam's score if it generates a token same as any beam from other group\n",
      " |              at a particular time. Note that `diversity_penalty` is only effective if `group beam search` is\n",
      " |              enabled.\n",
      " |          prefix_allowed_tokens_fn (`Callable[[int, torch.Tensor], List[int]]`, *optional*):\n",
      " |              If provided, this function constraints the beam search to allowed tokens only at each step. If not\n",
      " |              provided no constraint is applied. This function takes 2 arguments: the batch ID `batch_id` and\n",
      " |              `input_ids`. It has to return a list with the allowed tokens for the next generation step conditioned\n",
      " |              on the batch ID `batch_id` and the previously generated tokens `inputs_ids`. This argument is useful\n",
      " |              for constrained generation conditioned on the prefix, as described in [Autoregressive Entity\n",
      " |              Retrieval](https://arxiv.org/abs/2010.00904).\n",
      " |          logits_processor (`LogitsProcessorList`, *optional*):\n",
      " |               Custom logits processors that complement the default logits processors built from arguments and a\n",
      " |               model's config. If a logit processor is passed that is already created with the arguments or a model's\n",
      " |               config an error is thrown. This feature is intended for advanced users.\n",
      " |          renormalize_logits: (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether to renormalize the logits after applying all the logits processors or warpers (including the\n",
      " |              custom ones). It's highly recommended to set this flag to `True` as the search algorithms suppose the\n",
      " |              score logits are normalized but some logit processors or warpers break the normalization.\n",
      " |          stopping_criteria (`StoppingCriteriaList`, *optional*):\n",
      " |               Custom stopping criteria that complement the default stopping criteria built from arguments and a\n",
      " |               model's config. If a stopping criteria is passed that is already created with the arguments or a\n",
      " |               model's config an error is thrown. This feature is intended for advanced users.\n",
      " |          constraints (`List[Constraint]`, *optional*):\n",
      " |               Custom constraints that can be added to the generation to ensure that the output will contain the use\n",
      " |               of certain tokens as defined by `Constraint` objects, in the most sensible way possible.\n",
      " |          output_attentions (`bool`, *optional*, defaults to `model.config.output_attentions` or `False` if the config does not set any value):\n",
      " |              Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
      " |              returned tensors for more details.\n",
      " |          output_hidden_states (`bool`, *optional*, defaults to `model.config.output_hidden_states` or `False` if the config does not set any value):\n",
      " |              Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
      " |              for more details.\n",
      " |          output_scores (`bool`, *optional*, defaults to `model.config.output_scores` or `False` if the config does not set any value):\n",
      " |              Whether or not to return the prediction scores. See `scores` under returned tensors for more details.\n",
      " |          return_dict_in_generate (`bool`, *optional*, defaults to `model.config.return_dict_in_generate` or `False` if the config does not set any value):\n",
      " |              Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
      " |          forced_bos_token_id (`int`, *optional*, defaults to `model.config.forced_bos_token_id`):\n",
      " |              The id of the token to force as the first generated token after the `decoder_start_token_id`. Useful\n",
      " |              for multilingual models like [mBART](../model_doc/mbart) where the first generated token needs to be\n",
      " |              the target language token.\n",
      " |          forced_eos_token_id (`int`, *optional*, defaults to `model.config.forced_eos_token_id`):\n",
      " |              The id of the token to force as the last generated token when `max_length` is reached.\n",
      " |          remove_invalid_values (`bool`, *optional*, defaults to `model.config.remove_invalid_values`):\n",
      " |              Whether to remove possible *nan* and *inf* outputs of the model to prevent the generation method to\n",
      " |              crash. Note that using `remove_invalid_values` can slow down generation.\n",
      " |          synced_gpus (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n",
      " |          exponential_decay_length_penalty (`tuple(int, float)`, *optional*, defaults to `model.config.exponential_decay_length_penalty`):\n",
      " |              This Tuple adds an exponentially increasing length penalty, after a certain amount of tokens have been\n",
      " |              generated. The tuple shall consist of: `(start_index, decay_factor)` where `start_index` indicates\n",
      " |              where penalty starts and `decay_factor` represents the factor of exponential decay\n",
      " |      \n",
      " |          model_kwargs:\n",
      " |              Additional model specific kwargs will be forwarded to the `forward` function of the model. If the model\n",
      " |              is an encoder-decoder model, encoder specific kwargs should not be prefixed and decoder specific kwargs\n",
      " |              should be prefixed with *decoder_*.\n",
      " |      \n",
      " |      Return:\n",
      " |          [`~utils.ModelOutput`] or `torch.LongTensor`: A [`~utils.ModelOutput`] (if `return_dict_in_generate=True`\n",
      " |          or when `config.return_dict_in_generate=True`) or a `torch.FloatTensor`.\n",
      " |      \n",
      " |              If the model is *not* an encoder-decoder model (`model.config.is_encoder_decoder=False`), the possible\n",
      " |              [`~utils.ModelOutput`] types are:\n",
      " |      \n",
      " |                  - [`~generation_utils.GreedySearchDecoderOnlyOutput`],\n",
      " |                  - [`~generation_utils.SampleDecoderOnlyOutput`],\n",
      " |                  - [`~generation_utils.BeamSearchDecoderOnlyOutput`],\n",
      " |                  - [`~generation_utils.BeamSampleDecoderOnlyOutput`]\n",
      " |      \n",
      " |              If the model is an encoder-decoder model (`model.config.is_encoder_decoder=True`), the possible\n",
      " |              [`~utils.ModelOutput`] types are:\n",
      " |      \n",
      " |                  - [`~generation_utils.GreedySearchEncoderDecoderOutput`],\n",
      " |                  - [`~generation_utils.SampleEncoderDecoderOutput`],\n",
      " |                  - [`~generation_utils.BeamSearchEncoderDecoderOutput`],\n",
      " |                  - [`~generation_utils.BeamSampleEncoderDecoderOutput`]\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      Greedy Decoding:\n",
      " |      \n",
      " |      ```python\n",
      " |      >>> from transformers import AutoTokenizer, AutoModelForCausalLM\n",
      " |      \n",
      " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
      " |      >>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
      " |      \n",
      " |      >>> prompt = \"Today I believe we can finally\"\n",
      " |      >>> input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
      " |      \n",
      " |      >>> # generate up to 30 tokens\n",
      " |      >>> outputs = model.generate(input_ids, do_sample=False, max_length=30)\n",
      " |      >>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
      " |      ['Today I believe we can finally get to the point where we can make a difference in the lives of the people of the United States of America.\\n']\n",
      " |      ```\n",
      " |      \n",
      " |      Multinomial Sampling:\n",
      " |      \n",
      " |      ```python\n",
      " |      >>> from transformers import AutoTokenizer, AutoModelForCausalLM\n",
      " |      >>> import torch\n",
      " |      \n",
      " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
      " |      >>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
      " |      \n",
      " |      >>> prompt = \"Today I believe we can finally\"\n",
      " |      >>> input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
      " |      \n",
      " |      >>> # sample up to 30 tokens\n",
      " |      >>> torch.manual_seed(0)  # doctest: +IGNORE_RESULT\n",
      " |      >>> outputs = model.generate(input_ids, do_sample=True, max_length=30)\n",
      " |      >>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
      " |      ['Today I believe we can finally get rid of discrimination,\" said Rep. Mark Pocan (D-Wis.).\\n\\n\"Just look at the']\n",
      " |      ```\n",
      " |      \n",
      " |      Beam-search decoding:\n",
      " |      \n",
      " |      ```python\n",
      " |      >>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
      " |      \n",
      " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n",
      " |      >>> model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n",
      " |      \n",
      " |      >>> sentence = \"Paris is one of the densest populated areas in Europe.\"\n",
      " |      >>> input_ids = tokenizer(sentence, return_tensors=\"pt\").input_ids\n",
      " |      \n",
      " |      >>> outputs = model.generate(input_ids, num_beams=5)\n",
      " |      >>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
      " |      ['Paris ist eines der dichtesten besiedelten Gebiete Europas.']\n",
      " |      ```\n",
      " |  \n",
      " |  greedy_search(self, input_ids: torch.LongTensor, logits_processor: Optional[transformers.generation_logits_process.LogitsProcessorList] = None, stopping_criteria: Optional[transformers.generation_stopping_criteria.StoppingCriteriaList] = None, max_length: Optional[int] = None, pad_token_id: Optional[int] = None, eos_token_id: Optional[int] = None, output_attentions: Optional[bool] = None, output_hidden_states: Optional[bool] = None, output_scores: Optional[bool] = None, return_dict_in_generate: Optional[bool] = None, synced_gpus: Optional[bool] = False, **model_kwargs) -> Union[transformers.generation_utils.GreedySearchEncoderDecoderOutput, transformers.generation_utils.GreedySearchDecoderOnlyOutput, torch.LongTensor]\n",
      " |      Generates sequences of token ids for models with a language modeling head using **greedy decoding** and can be\n",
      " |      used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      " |              The sequence used as a prompt for the generation.\n",
      " |          logits_processor (`LogitsProcessorList`, *optional*):\n",
      " |              An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsProcessor`]\n",
      " |              used to modify the prediction scores of the language modeling head applied at each generation step.\n",
      " |          stopping_criteria (`StoppingCriteriaList`, *optional*):\n",
      " |              An instance of [`StoppingCriteriaList`]. List of instances of class derived from [`StoppingCriteria`]\n",
      " |              used to tell if the generation loop should stop.\n",
      " |      \n",
      " |          max_length (`int`, *optional*, defaults to 20):\n",
      " |              **DEPRECATED**. Use `logits_processor` or `stopping_criteria` directly to cap the number of generated\n",
      " |              tokens. The maximum length of the sequence to be generated.\n",
      " |          pad_token_id (`int`, *optional*):\n",
      " |              The id of the *padding* token.\n",
      " |          eos_token_id (`int`, *optional*):\n",
      " |              The id of the *end-of-sequence* token.\n",
      " |          output_attentions (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
      " |              returned tensors for more details.\n",
      " |          output_hidden_states (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
      " |              for more details.\n",
      " |          output_scores (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the prediction scores. See `scores` under returned tensors for more details.\n",
      " |          return_dict_in_generate (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
      " |          synced_gpus (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n",
      " |          model_kwargs:\n",
      " |              Additional model specific keyword arguments will be forwarded to the `forward` function of the model.\n",
      " |              If model is an encoder-decoder model the kwargs should include `encoder_outputs`.\n",
      " |      \n",
      " |      Return:\n",
      " |          [`~generation_utils.GreedySearchDecoderOnlyOutput`], [`~generation_utils.GreedySearchEncoderDecoderOutput`]\n",
      " |          or `torch.LongTensor`: A `torch.LongTensor` containing the generated tokens (default behaviour) or a\n",
      " |          [`~generation_utils.GreedySearchDecoderOnlyOutput`] if `model.config.is_encoder_decoder=False` and\n",
      " |          `return_dict_in_generate=True` or a [`~generation_utils.GreedySearchEncoderDecoderOutput`] if\n",
      " |          `model.config.is_encoder_decoder=True`.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      >>> from transformers import (\n",
      " |      ...     AutoTokenizer,\n",
      " |      ...     AutoModelForCausalLM,\n",
      " |      ...     LogitsProcessorList,\n",
      " |      ...     MinLengthLogitsProcessor,\n",
      " |      ...     StoppingCriteriaList,\n",
      " |      ...     MaxLengthCriteria,\n",
      " |      ... )\n",
      " |      \n",
      " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
      " |      >>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
      " |      \n",
      " |      >>> # set pad_token_id to eos_token_id because GPT2 does not have a PAD token\n",
      " |      >>> model.config.pad_token_id = model.config.eos_token_id\n",
      " |      \n",
      " |      >>> input_prompt = \"It might be possible to\"\n",
      " |      >>> input_ids = tokenizer(input_prompt, return_tensors=\"pt\").input_ids\n",
      " |      \n",
      " |      >>> # instantiate logits processors\n",
      " |      >>> logits_processor = LogitsProcessorList(\n",
      " |      ...     [\n",
      " |      ...         MinLengthLogitsProcessor(10, eos_token_id=model.config.eos_token_id),\n",
      " |      ...     ]\n",
      " |      ... )\n",
      " |      >>> stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=20)])\n",
      " |      \n",
      " |      >>> outputs = model.greedy_search(\n",
      " |      ...     input_ids, logits_processor=logits_processor, stopping_criteria=stopping_criteria\n",
      " |      ... )\n",
      " |      \n",
      " |      >>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
      " |      [\"It might be possible to get a better understanding of the nature of the problem, but it's not\"]\n",
      " |      ```\n",
      " |  \n",
      " |  group_beam_search(self, input_ids: torch.LongTensor, beam_scorer: transformers.generation_beam_search.BeamScorer, logits_processor: Optional[transformers.generation_logits_process.LogitsProcessorList] = None, stopping_criteria: Optional[transformers.generation_stopping_criteria.StoppingCriteriaList] = None, max_length: Optional[int] = None, pad_token_id: Optional[int] = None, eos_token_id: Optional[int] = None, output_attentions: Optional[bool] = None, output_hidden_states: Optional[bool] = None, output_scores: Optional[bool] = None, return_dict_in_generate: Optional[bool] = None, synced_gpus: Optional[bool] = False, **model_kwargs)\n",
      " |      Generates sequences of token ids for models with a language modeling head using **diverse beam search\n",
      " |      decoding** and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      " |              The sequence used as a prompt for the generation.\n",
      " |          beam_scorer (`BeamScorer`):\n",
      " |              An derived instance of [`BeamScorer`] that defines how beam hypotheses are constructed, stored and\n",
      " |              sorted during generation. For more information, the documentation of [`BeamScorer`] should be read.\n",
      " |          logits_processor (`LogitsProcessorList`, *optional*):\n",
      " |              An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsProcessor`]\n",
      " |              used to modify the prediction scores of the language modeling head applied at each generation step.\n",
      " |          stopping_criteria (`StoppingCriteriaList`, *optional*):\n",
      " |              An instance of [`StoppingCriteriaList`]. List of instances of class derived from [`StoppingCriteria`]\n",
      " |              used to tell if the generation loop should stop.\n",
      " |          max_length (`int`, *optional*, defaults to 20):\n",
      " |              **DEPRECATED**. Use `logits_processor` or `stopping_criteria` directly to cap the number of generated\n",
      " |              tokens. The maximum length of the sequence to be generated.\n",
      " |          pad_token_id (`int`, *optional*):\n",
      " |              The id of the *padding* token.\n",
      " |          eos_token_id (`int`, *optional*):\n",
      " |              The id of the *end-of-sequence* token.\n",
      " |          output_attentions (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
      " |              returned tensors for more details.\n",
      " |          output_hidden_states (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
      " |              for more details.\n",
      " |          output_scores (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the prediction scores. See `scores` under returned tensors for more details.\n",
      " |          return_dict_in_generate (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
      " |          synced_gpus (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n",
      " |      \n",
      " |          model_kwargs:\n",
      " |              Additional model specific kwargs that will be forwarded to the `forward` function of the model. If\n",
      " |              model is an encoder-decoder model the kwargs should include `encoder_outputs`.\n",
      " |      \n",
      " |      Return:\n",
      " |          [`~generation_utils.BeamSearchDecoderOnlyOutput`], [`~generation_utils.BeamSearchEncoderDecoderOutput`] or\n",
      " |          `torch.LongTensor`: A `torch.LongTensor` containing the generated tokens (default behaviour) or a\n",
      " |          [`~generation_utils.BeamSearchDecoderOnlyOutput`] if [`~generation_utils.BeamSearchDecoderOnlyOutput`] if\n",
      " |          `model.config.is_encoder_decoder=False` and `return_dict_in_generate=True` or a\n",
      " |          [`~generation_utils.BeamSearchEncoderDecoderOutput`] if `model.config.is_encoder_decoder=True`.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      >>> from transformers import (\n",
      " |      ...     AutoTokenizer,\n",
      " |      ...     AutoModelForSeq2SeqLM,\n",
      " |      ...     LogitsProcessorList,\n",
      " |      ...     MinLengthLogitsProcessor,\n",
      " |      ...     HammingDiversityLogitsProcessor,\n",
      " |      ...     BeamSearchScorer,\n",
      " |      ... )\n",
      " |      >>> import torch\n",
      " |      \n",
      " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
      " |      >>> model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
      " |      \n",
      " |      >>> encoder_input_str = \"translate English to German: How old are you?\"\n",
      " |      >>> encoder_input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids\n",
      " |      \n",
      " |      \n",
      " |      >>> # lets run diverse beam search using 6 beams\n",
      " |      >>> num_beams = 6\n",
      " |      >>> # define decoder start token ids\n",
      " |      >>> input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)\n",
      " |      >>> input_ids = input_ids * model.config.decoder_start_token_id\n",
      " |      \n",
      " |      >>> # add encoder_outputs to model keyword arguments\n",
      " |      >>> model_kwargs = {\n",
      " |      ...     \"encoder_outputs\": model.get_encoder()(\n",
      " |      ...         encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True\n",
      " |      ...     )\n",
      " |      ... }\n",
      " |      \n",
      " |      >>> # instantiate beam scorer\n",
      " |      >>> beam_scorer = BeamSearchScorer(\n",
      " |      ...     batch_size=1,\n",
      " |      ...     max_length=model.config.max_length,\n",
      " |      ...     num_beams=num_beams,\n",
      " |      ...     device=model.device,\n",
      " |      ...     num_beam_groups=3,\n",
      " |      ... )\n",
      " |      \n",
      " |      >>> # instantiate logits processors\n",
      " |      >>> logits_processor = LogitsProcessorList(\n",
      " |      ...     [\n",
      " |      ...         HammingDiversityLogitsProcessor(5.5, num_beams=6, num_beam_groups=3),\n",
      " |      ...         MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),\n",
      " |      ...     ]\n",
      " |      ... )\n",
      " |      \n",
      " |      >>> outputs = model.group_beam_search(\n",
      " |      ...     input_ids, beam_scorer, logits_processor=logits_processor, **model_kwargs\n",
      " |      ... )\n",
      " |      \n",
      " |      >>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
      " |      ['Wie alt bist du?']\n",
      " |      ```\n",
      " |  \n",
      " |  sample(self, input_ids: torch.LongTensor, logits_processor: Optional[transformers.generation_logits_process.LogitsProcessorList] = None, stopping_criteria: Optional[transformers.generation_stopping_criteria.StoppingCriteriaList] = None, logits_warper: Optional[transformers.generation_logits_process.LogitsProcessorList] = None, max_length: Optional[int] = None, pad_token_id: Optional[int] = None, eos_token_id: Optional[int] = None, output_attentions: Optional[bool] = None, output_hidden_states: Optional[bool] = None, output_scores: Optional[bool] = None, return_dict_in_generate: Optional[bool] = None, synced_gpus: Optional[bool] = False, **model_kwargs) -> Union[transformers.generation_utils.SampleEncoderDecoderOutput, transformers.generation_utils.SampleDecoderOnlyOutput, torch.LongTensor]\n",
      " |      Generates sequences of token ids for models with a language modeling head using **multinomial sampling** and\n",
      " |      can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      " |              The sequence used as a prompt for the generation.\n",
      " |          logits_processor (`LogitsProcessorList`, *optional*):\n",
      " |              An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsProcessor`]\n",
      " |              used to modify the prediction scores of the language modeling head applied at each generation step.\n",
      " |          stopping_criteria (`StoppingCriteriaList`, *optional*):\n",
      " |              An instance of [`StoppingCriteriaList`]. List of instances of class derived from [`StoppingCriteria`]\n",
      " |              used to tell if the generation loop should stop.\n",
      " |          logits_warper (`LogitsProcessorList`, *optional*):\n",
      " |              An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsWarper`] used\n",
      " |              to warp the prediction score distribution of the language modeling head applied before multinomial\n",
      " |              sampling at each generation step.\n",
      " |          max_length (`int`, *optional*, defaults to 20):\n",
      " |              **DEPRECATED**. Use `logits_processor` or `stopping_criteria` directly to cap the number of generated\n",
      " |              tokens. The maximum length of the sequence to be generated.\n",
      " |          pad_token_id (`int`, *optional*):\n",
      " |              The id of the *padding* token.\n",
      " |          eos_token_id (`int`, *optional*):\n",
      " |              The id of the *end-of-sequence* token.\n",
      " |          output_attentions (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
      " |              returned tensors for more details.\n",
      " |          output_hidden_states (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
      " |              for more details.\n",
      " |          output_scores (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the prediction scores. See `scores` under returned tensors for more details.\n",
      " |          return_dict_in_generate (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
      " |          synced_gpus (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n",
      " |          model_kwargs:\n",
      " |              Additional model specific kwargs will be forwarded to the `forward` function of the model. If model is\n",
      " |              an encoder-decoder model the kwargs should include `encoder_outputs`.\n",
      " |      \n",
      " |      Return:\n",
      " |          [`~generation_utils.SampleDecoderOnlyOutput`], [`~generation_utils.SampleEncoderDecoderOutput`] or\n",
      " |          `torch.LongTensor`: A `torch.LongTensor` containing the generated tokens (default behaviour) or a\n",
      " |          [`~generation_utils.SampleDecoderOnlyOutput`] if `model.config.is_encoder_decoder=False` and\n",
      " |          `return_dict_in_generate=True` or a [`~generation_utils.SampleEncoderDecoderOutput`] if\n",
      " |          `model.config.is_encoder_decoder=True`.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      >>> from transformers import (\n",
      " |      ...     AutoTokenizer,\n",
      " |      ...     AutoModelForCausalLM,\n",
      " |      ...     LogitsProcessorList,\n",
      " |      ...     MinLengthLogitsProcessor,\n",
      " |      ...     TopKLogitsWarper,\n",
      " |      ...     TemperatureLogitsWarper,\n",
      " |      ...     StoppingCriteriaList,\n",
      " |      ...     MaxLengthCriteria,\n",
      " |      ... )\n",
      " |      >>> import torch\n",
      " |      \n",
      " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
      " |      >>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
      " |      \n",
      " |      >>> # set pad_token_id to eos_token_id because GPT2 does not have a EOS token\n",
      " |      >>> model.config.pad_token_id = model.config.eos_token_id\n",
      " |      \n",
      " |      >>> input_prompt = \"Today is a beautiful day, and\"\n",
      " |      >>> input_ids = tokenizer(input_prompt, return_tensors=\"pt\").input_ids\n",
      " |      \n",
      " |      >>> # instantiate logits processors\n",
      " |      >>> logits_processor = LogitsProcessorList(\n",
      " |      ...     [\n",
      " |      ...         MinLengthLogitsProcessor(15, eos_token_id=model.config.eos_token_id),\n",
      " |      ...     ]\n",
      " |      ... )\n",
      " |      >>> # instantiate logits processors\n",
      " |      >>> logits_warper = LogitsProcessorList(\n",
      " |      ...     [\n",
      " |      ...         TopKLogitsWarper(50),\n",
      " |      ...         TemperatureLogitsWarper(0.7),\n",
      " |      ...     ]\n",
      " |      ... )\n",
      " |      \n",
      " |      >>> stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=20)])\n",
      " |      \n",
      " |      >>> torch.manual_seed(0)  # doctest: +IGNORE_RESULT\n",
      " |      >>> outputs = model.sample(\n",
      " |      ...     input_ids,\n",
      " |      ...     logits_processor=logits_processor,\n",
      " |      ...     logits_warper=logits_warper,\n",
      " |      ...     stopping_criteria=stopping_criteria,\n",
      " |      ... )\n",
      " |      \n",
      " |      >>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
      " |      ['Today is a beautiful day, and a wonderful day.\\n\\nI was lucky enough to meet the']\n",
      " |      ```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model.train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('test': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "beeac2e85e13a42cd30369aae72be3991bad01c857e8f39f76b9988b0534510b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
