{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install transformers --quiet\n",
    "# !pip3 install datasets --quiet\n",
    "# !pip3 install torch --quiet\n",
    "# !pip3 install sentencepiece --quiet\n",
    "# !pip3 install protobuf==3.20 --quiet\n",
    "# !pip3 install pillow --quiet\n",
    "# !pip3 install GPUtil --quiet\n",
    "# !pip3 install nvidia-smi --quiet\n",
    "# !pip3 install diffusers==0.3.0 transformers scipy ftfy --quiet\n",
    "# !pip3 install huggingface_hub --quiet\n",
    "# !python -m pip install huggingface_hub --quiet\n",
    "# !pip3 install \"ipywidgets>=7,<8\" --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from GPUtil import showUtilization as gpu_usage\n",
    "# gpu_usage()  \n",
    "\n",
    "\n",
    "#!#!#!#!#!#!#!#!#!#!#!#!#!#!#!#!#!#!#!#!#!#!#!#\n",
    "#!  Libraries Imported Below For Each Model  !#\n",
    "#!#!#!#!#!#!#!#!#!#!#!#!#!#!#!#!#!#!#!#!#!#!#!#\n",
    "\n",
    "\n",
    "#Import libraries\n",
    "# from transformers import AutoModelForSequenceClassification,AutoConfig,AutoModelWithLMHead, AutoTokenizer, VisionEncoderDecoderModel, ViTFeatureExtractor\n",
    "# from diffusers import StableDiffusionPipeline\n",
    "# import torch\n",
    "# from torch import autocast\n",
    "# from PIL import Image\n",
    "# import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from huggingface_hub import notebook_login\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stable Diffusion\n",
    "\n",
    "Stable diffusion starts by programatically applying noise to an image and \"learns\" the steps to go in the reverse direction to remove to noise. That is, the model learns how to get from the latent space (representation of data) to a clear image. Since the latent space is very abstract, many different kinds of objects can exist in this space. Objects such as pictures and text are able to be represented. Since the model knows how to work backwards from the latent space to a clear image, the model can take text (or other images) and \"work backwards\" to produce and image. \n",
    "<br/>\n",
    "<center><img src=\"ld.PNG\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7af44e14bf134ff3b57692ad36d11e50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import autocast\n",
    "from diffusers import StableDiffusionPipeline\n",
    "notebook_login()\n",
    "\n",
    "class stable_diff:\n",
    "    def __init__(self,demo=False):\n",
    "        if not demo:\n",
    "            self.prompt = input(\"Enter a prompt for the model: \")\n",
    "        else:\n",
    "            self.prompt = \"a photo of an astronaut riding a horse on mars\"\n",
    "\n",
    "    def output(self):\n",
    "        model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "        device = \"cuda\"\n",
    "        pipe = StableDiffusionPipeline.from_pretrained(model_id, use_auth_token=True)\n",
    "        pipe = pipe.to(device)\n",
    "\n",
    "        with autocast(\"cuda\"):\n",
    "            image = pipe(self.prompt, guidance_scale=5.5).images[0]  \n",
    "\n",
    "        print(f'The prompt was \"{self.prompt}\"')    \n",
    "        image.show()\n",
    "        image.save(f'pictures/{self.prompt}.png')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image2Text\n",
    "\n",
    "GPT-2 model pre-trained on captioned images. GPT-2 is a 12 layer decoder only transformer architecture with 1.5 billion parameters. The model is trained in a self-supervised manner using the masked language model objective. A large block of text is broken into smaller segments, and individual words in the sequnce are masked. The model reads the sequence left-to-right and right-to-left and tries to learn context to predict each masked words based on the words around it. Large pre-trained language models can be fine-tuned to achieve high performance in particular domains. This particular model is only the pre-trained version.\n",
    "\n",
    "The model takes images as input and returns a caption for each image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer\n",
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class img2txt:\n",
    "  def __init__(self):\n",
    "    curr_dir = os.getcwd()\n",
    "    curr_dir += '/pictures/'\n",
    "    self.files = list(os.walk(curr_dir))[0][2]\n",
    "\n",
    "  def predict(self):\n",
    "\n",
    "    checkpoint = \"nlpconnect/vit-gpt2-image-captioning\"\n",
    "\n",
    "    model = VisionEncoderDecoderModel.from_pretrained(checkpoint)\n",
    "    feature_extractor = ViTFeatureExtractor.from_pretrained(checkpoint)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "    max_length = 16\n",
    "    num_beams = 4\n",
    "    gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}\n",
    "    def predict_step(image_paths):\n",
    "      images = []\n",
    "      for image_path in image_paths:\n",
    "        i_image = Image.open(image_path)\n",
    "        if i_image.mode != \"RGB\":\n",
    "          i_image = i_image.convert(mode=\"RGB\")\n",
    "\n",
    "        images.append(i_image)\n",
    "\n",
    "      pixel_values = feature_extractor(images=images, return_tensors=\"pt\").pixel_values\n",
    "      pixel_values = pixel_values.to(device)\n",
    "\n",
    "      output_ids = model.generate(pixel_values, **gen_kwargs)\n",
    "\n",
    "      preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "      preds = [pred.strip() for pred in preds]\n",
    "      return preds\n",
    "\n",
    "\n",
    "    for fil in self.files:\n",
    "      im = Image.open(f'./pictures/{fil}')\n",
    "      im.show()\n",
    "      print(predict_step([f'./pictures/{fil}']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Shot-Classification\n",
    "\n",
    "The Bart model is a sequence-to-sequence (encoder and decoder) model trained as a denoising autoencoder. Bart was trained on the MultiNLI dataset. In zero shot classification, the model predicts the label of examples it did not see during training. The model levrages its general understanding of language and calculates the probabilility that each label is true given the sequence. The model takes as input a word or phrase and several candidate labels, and returns a classification chosen from the candidate labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer,AutoConfig\n",
    "\n",
    "class zero_shot:\n",
    "    def __init__(self,demo=False):\n",
    "        if not demo:\n",
    "            # User enters the text they want classified as well as any number of possible labels\n",
    "            self.premise = input(\"Please enter some text to be classified\")\n",
    "            self.num_labels = input('Enter the number of labels')\n",
    "            self.labels = [input(f\"Please enter the number{i+1} label\") for i in range(int(self.num_labels))]\n",
    "        else:\n",
    "            self.premise = \"I have a problem with my iphone that needs to be resolved asap!!\"\n",
    "            self.num_labels = 3\n",
    "            self.labels = [\"urgent\",\"not urgent\",\"phone\",\"tablet\",\"computer\"]\n",
    "    \n",
    "    def predict(self):\n",
    "\n",
    "        # Start with checkpoint \"bart-large-mnli\"\n",
    "        checkpoint = 'facebook/bart-large-mnli'\n",
    "        checkpoint2 = \"joeddav/xlm-roberta-large-xnli\"\n",
    "\n",
    "        # Load the model and tokenizer with pre-trained weights from the checkpoint\n",
    "        nli_model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "        # Define empty dict that will be used for the output\n",
    "        output = {}\n",
    "\n",
    "        # For each label\n",
    "        for label in self.labels:\n",
    "            # Autotokenizer encodes text input with current label\n",
    "            tok = tokenizer.encode(self.premise, label, return_tensors='pt',truncation_strategy='only_first')  \n",
    "            \n",
    "            # classify tokenized data \n",
    "            logits = nli_model(tok)[0]\n",
    "\n",
    "            # retrieve the probability that the current label is correct\n",
    "            entail_contradiction_logits = logits[:,[0,2]]\n",
    "            probs1 = entail_contradiction_logits.softmax(dim=1)\n",
    "            output[label] = float(probs1[:,1])\n",
    "\n",
    "        # Select the hist probability (value)\n",
    "        max_val = max(output.values())\n",
    "\n",
    "        # Print the premise and the key (label) corresponding to the highest probability\n",
    "        print('The premise is:')\n",
    "        print(self.premise)\n",
    "        print('')\n",
    "        # Print the label corresponding to the highest probability\n",
    "        print(next(k for k,v in output.items() if v == max_val))\n",
    "        print('')\n",
    "        print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer Questioning (Jeopardy Model)\n",
    "\n",
    "The t5 an encoder-decoder model that was pretrained on a large corpus and fine tuned on the Squad 1.1 dataset for question generation. t5 is a sequence to sequence model. This model takes as input the answer to a hypothetical question, and some context upon which to formulate a question. The model returns the question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelWithLMHead\n",
    "\n",
    "class answer_question:\n",
    "    def __init__(self,demo=False):\n",
    "        if not demo:\n",
    "            self.answer = input(\"Enter The Answer to the Question\")\n",
    "            self.context = input(\"Provide Some Context\")\n",
    "        else:\n",
    "            self.context = \"Manuel has created RuPERTa-base with the support of HF-Transformers and Google\"\n",
    "            self.answer = \"Manuel\"\n",
    "\n",
    "    def get_question(self,max_length=64):\n",
    "        checkpoint = \"mrm8488/t5-base-finetuned-question-generation-ap\"\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "        model = AutoModelWithLMHead.from_pretrained(checkpoint)\n",
    "\n",
    "        input_text = \"answer: %s  context: %s </s>\" % (self.answer, self.context)\n",
    "        features = tokenizer([input_text], return_tensors='pt')\n",
    "\n",
    "        output = model.generate(input_ids=features['input_ids'], \n",
    "                    attention_mask=features['attention_mask'],\n",
    "                    max_length=max_length)\n",
    "\n",
    "        x = tokenizer.decode(output[0])\n",
    "        print(f\"The context is: {self.context}\")\n",
    "        print(f\"The answer to the question is: {self.answer}\")\n",
    "        print(f\"The question is: {x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The premise is:\n",
      "I have a problem with my iphone that needs to be resolved asap!!\n",
      "\n",
      "urgent\n",
      "\n",
      "{'urgent': 0.9913657903671265, 'not urgent': 0.00046831939835101366, 'phone': 0.9534575343132019, 'tablet': 0.03839779272675514, 'computer': 0.14123128354549408}\n"
     ]
    }
   ],
   "source": [
    "zero_shot(demo=True).predict()\n",
    "# zero_shot().predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The context is: we can mitigate the risks of artificial intelliogence by researching and building our own artificial intelliogence\n",
      "The answer to the question is: artificial intelligence\n",
      "The question is: <pad> question: What type of intelliogence can we mitigate the risks of?</s>\n"
     ]
    }
   ],
   "source": [
    "# answer_question(demo=True).get_question()\n",
    "answer_question().get_question()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img2txt().predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "You specified use_auth_token=True, but a Hugging Face token was not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/home/jmunse/workspace/models_demo.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.137.224.81/home/jmunse/workspace/models_demo.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m stable_diff()\u001b[39m.\u001b[39;49moutput()\n",
      "\u001b[1;32m/home/jmunse/workspace/models_demo.ipynb Cell 16\u001b[0m in \u001b[0;36mstable_diff.output\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.137.224.81/home/jmunse/workspace/models_demo.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m device \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.137.224.81/home/jmunse/workspace/models_demo.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# access_token = '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'\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.137.224.81/home/jmunse/workspace/models_demo.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m pipe \u001b[39m=\u001b[39m StableDiffusionPipeline\u001b[39m.\u001b[39;49mfrom_pretrained(model_id, use_auth_token\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.137.224.81/home/jmunse/workspace/models_demo.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m pipe \u001b[39m=\u001b[39m pipe\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.137.224.81/home/jmunse/workspace/models_demo.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mwith\u001b[39;00m autocast(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.9/site-packages/diffusers/pipeline_utils.py:288\u001b[0m, in \u001b[0;36mDiffusionPipeline.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[39m# 1. Download the checkpoints and configs\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[39m# use snapshot download here to get it working from from_pretrained\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(pretrained_model_name_or_path):\n\u001b[0;32m--> 288\u001b[0m     cached_folder \u001b[39m=\u001b[39m snapshot_download(\n\u001b[1;32m    289\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m    290\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    291\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    292\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    293\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    294\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    295\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    296\u001b[0m     )\n\u001b[1;32m    297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m     cached_folder \u001b[39m=\u001b[39m pretrained_model_name_or_path\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:93\u001b[0m, in \u001b[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m         message \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m custom_message\n\u001b[1;32m     92\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(message, \u001b[39mFutureWarning\u001b[39;00m)\n\u001b[0;32m---> 93\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.9/site-packages/huggingface_hub/_snapshot_download.py:115\u001b[0m, in \u001b[0;36msnapshot_download\u001b[0;34m(repo_id, revision, repo_type, cache_dir, library_name, library_version, user_agent, proxies, etag_timeout, resume_download, use_auth_token, local_files_only, allow_regex, ignore_regex, allow_patterns, ignore_patterns)\u001b[0m\n\u001b[1;32m    113\u001b[0m     token \u001b[39m=\u001b[39m HfFolder\u001b[39m.\u001b[39mget_token()\n\u001b[1;32m    114\u001b[0m     \u001b[39mif\u001b[39;00m token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    116\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mYou specified use_auth_token=True, but a Hugging Face token was not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    117\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m found.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    118\u001b[0m         )\n\u001b[1;32m    119\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     token \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: You specified use_auth_token=True, but a Hugging Face token was not found."
     ]
    }
   ],
   "source": [
    "stable_diff().output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "beeac2e85e13a42cd30369aae72be3991bad01c857e8f39f76b9988b0534510b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
