{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /home/jmunse/anaconda3/envs/test/lib/python3.9/site-packages (0.9.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/jmunse/anaconda3/envs/test/lib/python3.9/site-packages (from huggingface_hub) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/jmunse/anaconda3/envs/test/lib/python3.9/site-packages (from huggingface_hub) (4.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/jmunse/anaconda3/envs/test/lib/python3.9/site-packages (from huggingface_hub) (21.3)\n",
      "Requirement already satisfied: tqdm in /home/jmunse/anaconda3/envs/test/lib/python3.9/site-packages (from huggingface_hub) (4.64.1)\n",
      "Requirement already satisfied: requests in /home/jmunse/anaconda3/envs/test/lib/python3.9/site-packages (from huggingface_hub) (2.28.1)\n",
      "Requirement already satisfied: filelock in /home/jmunse/anaconda3/envs/test/lib/python3.9/site-packages (from huggingface_hub) (3.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/jmunse/anaconda3/envs/test/lib/python3.9/site-packages (from packaging>=20.9->huggingface_hub) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jmunse/anaconda3/envs/test/lib/python3.9/site-packages (from requests->huggingface_hub) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/jmunse/anaconda3/envs/test/lib/python3.9/site-packages (from requests->huggingface_hub) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jmunse/anaconda3/envs/test/lib/python3.9/site-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/jmunse/anaconda3/envs/test/lib/python3.9/site-packages (from requests->huggingface_hub) (1.26.12)\n"
     ]
    }
   ],
   "source": [
    "# !pip3 install transformers --quiet\n",
    "# !pip3 install datasets --quiet\n",
    "# !pip3 install torch --quiet\n",
    "# !pip3 install sentencepiece --quiet\n",
    "# !pip3 install protobuf==3.20 --quiet\n",
    "# !pip3 install pillow --quiet\n",
    "# !pip3 install GPUtil --quiet\n",
    "# !pip3 install nvidia-smi --quiet\n",
    "# !pip3 install diffusers==0.3.0 transformers scipy ftfy --quiet\n",
    "# !pip3 install huggingface_hub --quiet\n",
    "# !python -m pip install huggingface_hub\n",
    "# !pip3 install \"ipywidgets>=7,<8\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 35% | 67% |\n",
      "|  1 | 42% | 32% |\n",
      "|  2 | 41% | 33% |\n",
      "|  3 | 14% | 33% |\n"
     ]
    }
   ],
   "source": [
    "from GPUtil import showUtilization as gpu_usage\n",
    "gpu_usage()  \n",
    "\n",
    "\n",
    "#!#!#!#!#!#!#!#!#!#!#!#!#!#!#!#!#!#!#!#!#!#!#!#\n",
    "#!  Libraries Imported Below For Each Model  !#\n",
    "#!#!#!#!#!#!#!#!#!#!#!#!#!#!#!#!#!#!#!#!#!#!#!#\n",
    "\n",
    "\n",
    "#Import libraries\n",
    "# from transformers import AutoModelForSequenceClassification,AutoConfig,AutoModelWithLMHead, AutoTokenizer, VisionEncoderDecoderModel, ViTFeatureExtractor\n",
    "# from diffusers import StableDiffusionPipeline\n",
    "# import torch\n",
    "# from torch import autocast\n",
    "# from PIL import Image\n",
    "# import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from huggingface_hub import notebook_login\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stable Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import autocast\n",
    "from diffusers import StableDiffusionPipeline\n",
    "notebook_login()\n",
    "\n",
    "class stable_diff:\n",
    "    def __init__(self,demo=False):\n",
    "        if not demo:\n",
    "            self.prompt = input(\"Enter a prompt for the model: \")\n",
    "        else:\n",
    "            self.prompt = \"a photo of an astronaut riding a horse on mars\"\n",
    "\n",
    "    def output(self):\n",
    "        model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "        device = \"cuda\"\n",
    "        # access_token = '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'\n",
    "        pipe = StableDiffusionPipeline.from_pretrained(model_id, use_auth_token=True)\n",
    "        pipe = pipe.to(device)\n",
    "\n",
    "        with autocast(\"cuda\"):\n",
    "            image = pipe(self.prompt, guidance_scale=5.5).images[0]  \n",
    "\n",
    "        print(f'The prompt was \"{self.prompt}\"')    \n",
    "        image.show()\n",
    "        image.save(f'pictures/{self.prompt}.png')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image2Text\n",
    "\n",
    "GPT-2 model pre-trained on captioned images. GPT-2 is a 12 layer decoder only transformer architecture with 1.5 billion parameters. The model is trained in a self-supervised manner using the masked language model objective. A large block of text is broken into smaller segments, and individual words in the sequnce are masked. The model reads the sequence left-to-right and right-to-left and tries to learn context to predict each masked words based on the words around it. Large pre-trained language models can be fine-tuned to achieve high performance in particular domains. This particular model is only the pre-trained version.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer\n",
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class img2txt:\n",
    "  def __init__(self):\n",
    "    curr_dir = os.getcwd()\n",
    "    curr_dir += '/pictures/'\n",
    "    self.files = list(os.walk(curr_dir))[0][2]\n",
    "\n",
    "  def predict(self):\n",
    "\n",
    "    checkpoint = \"nlpconnect/vit-gpt2-image-captioning\"\n",
    "\n",
    "    model = VisionEncoderDecoderModel.from_pretrained(checkpoint)\n",
    "    feature_extractor = ViTFeatureExtractor.from_pretrained(checkpoint)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "    max_length = 16\n",
    "    num_beams = 4\n",
    "    gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}\n",
    "    def predict_step(image_paths):\n",
    "      images = []\n",
    "      for image_path in image_paths:\n",
    "        i_image = Image.open(image_path)\n",
    "        if i_image.mode != \"RGB\":\n",
    "          i_image = i_image.convert(mode=\"RGB\")\n",
    "\n",
    "        images.append(i_image)\n",
    "\n",
    "      pixel_values = feature_extractor(images=images, return_tensors=\"pt\").pixel_values\n",
    "      pixel_values = pixel_values.to(device)\n",
    "\n",
    "      output_ids = model.generate(pixel_values, **gen_kwargs)\n",
    "\n",
    "      preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "      preds = [pred.strip() for pred in preds]\n",
    "      return preds\n",
    "\n",
    "\n",
    "    for fil in self.files:\n",
    "      im = Image.open(f'./pictures/{fil}')\n",
    "      im.show()\n",
    "      print(predict_step([f'./pictures/{fil}']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Shot-Classification\n",
    "\n",
    "The Bart model is a sequence-to-sequence (encoder and decoder) model trained as a denoising autoencoder. Bart was trained on   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer,AutoConfig\n",
    "\n",
    "class zero_shot:\n",
    "    def __init__(self,demo=False):\n",
    "        if not demo:\n",
    "            # User enters the text they want classified as well as any number of possible labels\n",
    "            self.premise = input(\"Please enter some text to be classified\")\n",
    "            self.num_labels = input('Enter the number of labels')\n",
    "            self.labels = [input(f\"Please enter the number{i+1} label\") for i in range(int(self.num_labels))]\n",
    "        else:\n",
    "            self.premise = \"I have a problem with my iphone that needs to be resolved asap!!\"\n",
    "            self.num_labels = 3\n",
    "            self.labels = [\"urgent\",\"not urgent\",\"phone\",\"tablet\",\"computer\"]\n",
    "    \n",
    "    def predict(self):\n",
    "\n",
    "        # Start with checkpoint \"bart-large-mnli\"\n",
    "        checkpoint = 'facebook/bart-large-mnli'\n",
    "        checkpoint2 = \"joeddav/xlm-roberta-large-xnli\"\n",
    "\n",
    "        # Load the model and tokenizer with pre-trained weights from the checkpoint\n",
    "        nli_model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "        # Define empty dict that will be used for the output\n",
    "        output = {}\n",
    "\n",
    "        # For each label\n",
    "        for label in self.labels:\n",
    "            # Autotokenizer encodes text input with current label\n",
    "            tok = tokenizer.encode(self.premise, label, return_tensors='pt',truncation_strategy='only_first')  \n",
    "            \n",
    "            # classify tokenized data \n",
    "            logits = nli_model(tok)[0]\n",
    "\n",
    "            # retrieve the probability that the current label is correct\n",
    "            entail_contradiction_logits = logits[:,[0,2]]\n",
    "            probs1 = entail_contradiction_logits.softmax(dim=1)\n",
    "            output[label] = float(probs1[:,1])\n",
    "\n",
    "        # Select the hist probability (value)\n",
    "        max_val = max(output.values())\n",
    "\n",
    "        # Print the premise and the key (label) corresponding to the highest probability\n",
    "        print('The premise is:')\n",
    "        print(self.premise)\n",
    "        print('')\n",
    "        # Print the label corresponding to the highest probability\n",
    "        print(next(k for k,v in output.items() if v == max_val))\n",
    "        print('')\n",
    "        print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer Questioning (Jeopardy Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelWithLMHead\n",
    "\n",
    "class answer_question:\n",
    "    def __init__(self,demo=False):\n",
    "        if not demo:\n",
    "            self.answer = input(\"Enter The Answer to the Question\")\n",
    "            self.context = input(\"Provide Some Context\")\n",
    "        else:\n",
    "            self.context = \"Manuel has created RuPERTa-base with the support of HF-Transformers and Google\"\n",
    "            self.answer = \"Manuel\"\n",
    "\n",
    "    def get_question(self,max_length=64):\n",
    "        checkpoint = \"mrm8488/t5-base-finetuned-question-generation-ap\"\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "        model = AutoModelWithLMHead.from_pretrained(checkpoint)\n",
    "\n",
    "        input_text = \"answer: %s  context: %s </s>\" % (self.answer, self.context)\n",
    "        features = tokenizer([input_text], return_tensors='pt')\n",
    "\n",
    "        output = model.generate(input_ids=features['input_ids'], \n",
    "                    attention_mask=features['attention_mask'],\n",
    "                    max_length=max_length)\n",
    "\n",
    "        x = tokenizer.decode(output[0])\n",
    "        print(f\"The context is: {self.context}\")\n",
    "        print(f\"The answer to the question is: {self.answer}\")\n",
    "        print(f\"The question is: {x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The premise is:\n",
      "I have a problem with my iphone that needs to be resolved asap!!\n",
      "\n",
      "urgent\n",
      "\n",
      "{'urgent': 0.9913657903671265, 'not urgent': 0.00046831939835101366, 'phone': 0.9534575343132019, 'tablet': 0.03839779272675514, 'computer': 0.14123128354549408}\n"
     ]
    }
   ],
   "source": [
    "zero_shot(demo=True).predict()\n",
    "# zero_shot().predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The context is: we can mitigate the risks of artificial intelliogence by researching and building our own artificial intelliogence\n",
      "The answer to the question is: artificial intelligence\n",
      "The question is: <pad> question: What type of intelliogence can we mitigate the risks of?</s>\n"
     ]
    }
   ],
   "source": [
    "# answer_question(demo=True).get_question()\n",
    "answer_question().get_question()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img2txt().predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "You specified use_auth_token=True, but a Hugging Face token was not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/home/jmunse/workspace/models_demo.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.137.224.81/home/jmunse/workspace/models_demo.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m stable_diff()\u001b[39m.\u001b[39;49moutput()\n",
      "\u001b[1;32m/home/jmunse/workspace/models_demo.ipynb Cell 16\u001b[0m in \u001b[0;36mstable_diff.output\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.137.224.81/home/jmunse/workspace/models_demo.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m device \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.137.224.81/home/jmunse/workspace/models_demo.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# access_token = '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'\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.137.224.81/home/jmunse/workspace/models_demo.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m pipe \u001b[39m=\u001b[39m StableDiffusionPipeline\u001b[39m.\u001b[39;49mfrom_pretrained(model_id, use_auth_token\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.137.224.81/home/jmunse/workspace/models_demo.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m pipe \u001b[39m=\u001b[39m pipe\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.137.224.81/home/jmunse/workspace/models_demo.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mwith\u001b[39;00m autocast(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.9/site-packages/diffusers/pipeline_utils.py:288\u001b[0m, in \u001b[0;36mDiffusionPipeline.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[39m# 1. Download the checkpoints and configs\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[39m# use snapshot download here to get it working from from_pretrained\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(pretrained_model_name_or_path):\n\u001b[0;32m--> 288\u001b[0m     cached_folder \u001b[39m=\u001b[39m snapshot_download(\n\u001b[1;32m    289\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m    290\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    291\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    292\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    293\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    294\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    295\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    296\u001b[0m     )\n\u001b[1;32m    297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m     cached_folder \u001b[39m=\u001b[39m pretrained_model_name_or_path\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:93\u001b[0m, in \u001b[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m         message \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m custom_message\n\u001b[1;32m     92\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(message, \u001b[39mFutureWarning\u001b[39;00m)\n\u001b[0;32m---> 93\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.9/site-packages/huggingface_hub/_snapshot_download.py:115\u001b[0m, in \u001b[0;36msnapshot_download\u001b[0;34m(repo_id, revision, repo_type, cache_dir, library_name, library_version, user_agent, proxies, etag_timeout, resume_download, use_auth_token, local_files_only, allow_regex, ignore_regex, allow_patterns, ignore_patterns)\u001b[0m\n\u001b[1;32m    113\u001b[0m     token \u001b[39m=\u001b[39m HfFolder\u001b[39m.\u001b[39mget_token()\n\u001b[1;32m    114\u001b[0m     \u001b[39mif\u001b[39;00m token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    116\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mYou specified use_auth_token=True, but a Hugging Face token was not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    117\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m found.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    118\u001b[0m         )\n\u001b[1;32m    119\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     token \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: You specified use_auth_token=True, but a Hugging Face token was not found."
     ]
    }
   ],
   "source": [
    "stable_diff().output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "beeac2e85e13a42cd30369aae72be3991bad01c857e8f39f76b9988b0534510b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
